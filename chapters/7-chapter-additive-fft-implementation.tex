%======================================================================
\chapter{Accelerating Post-quantum Secure zkSNARKs through Optimizing Additive FFT}\label{ch:additive-fft}
%======================================================================

\section*{Declaration of Contributions}
This chapter is based on [?]. I have co-authored the paper and my main contributions are as follows:
\begin{itemize}
	\item Deriving the exact number of additions and multiplications in the Cantor additive fast Fourier transform (\gls{fft}) \cite{Cantor1989FFT}. 
	\item The \CC implementation of all the Cantor additive \gls{fft} algorithm with the minimum number of additions.
	\item The dasign and implementation of precomputations in the presented additive \gls{fft}s.
	\item The \CC implementation of modified versions of the Gao-Mateer additive \gls{fft} \cite{Gao2010FFT}.
	\item  The \gls{fft} Complexity analysis of the Aurora zero-knowledge succinct non-interactive arguments of knowledge (\gls{zksnark}) and modified the Aurora  \CC  to employ  the Cantor additive \gls{fft} instead of  the Gao-Mateer additive \gls{fft}.
\end{itemize}

\section{Introduction}

The \gls{fft} over additive groups can be utilized in various \gls{zksnark} protocols operating over binary extension fields. Examples of such protocols include Ligero~\cite{Ames2017Ligero}, STARK~\cite{Ben-Sasson2018STARK}, Aurora~\cite{Aurora2019}, Fractal~\cite{Chiesa2020Fractal}, and Polaris~\cite{Polaris}. Among these \gls{zksnark}s, Fractal features the fastest verifier algorithm, Ligero boasts the fastest prover algorithm, and Aurora achieves the smallest proof size. While STARK employs algebraic intermediate representation (\gls{air}), which is designed for converting the execution trace of a program into algebraic representations (e.g., polynomials), the others rely on rank-1 constraint system (R1CS) \cite{Gong2024}, which is suited for arithmetic circuit and is preferred in cryptography related applications, as many privacy-preserving solutions rely on zero-knowledge proofs (\gls{zkp}s) of knowledge of the preimage of a leaf in a Merkle hash tree constructed by the circuit of a hash algorithm (e.g., SHA-256). Furthermore, \gls{zksnark}-based post-quantum digital signature schemes typically involve proving knowledge of the secret key for a symmetric-key encryption algorithm (e.g., AES) represented as an arithmetic circuit.

For this study, we select Aurora~\cite{Aurora2019} to demonstrate the performance improvements achieved by optimizing the \gls{fft} algorithm using the Cantor special basis \cite{Cantor1989FFT}. While our optimization is applicable to all the aforementioned \gls{zksnark}s, Aurora was chosen due to its small proof size, which makes it a strong candidate for post-quantum secure digital signature schemes. Accordingly, Aurora serves as the foundation of Preon~\cite{Preon2023}, a post-quantum digital signature scheme that was a first-round candidate in NISTâ€™s post-quantum cryptography standardization process~\cite{nist_pqc_round1_signatures}.

\begin{table}
	\caption[Prover and Verifier of the Aurora zkSNARK using Gao-Mateer and Cantor FFT algoritms]{ Prover and Verifier of the Aurora zkSNARK \cite{Aurora2019} over $\mathbb{F}_{2^{256}}$ based on the number of constraints $N$ and the size of the Reed-Solomon codeword domain $|L|$, using Gao-Mateer and Cantor FFTs, on an AMD Ryzen 9 9950x @ 5.7 GHz (sec).}
	\label{tab:cost_analysis_intorduction}
	\centering
	{\small
		\begin{tabularx}{\textwidth}{>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X>{\centering\arraybackslash}X}
			\toprule
			\multirow{3}{*}{$\log_2(N)$} & \multirow{3}{*}{$\log_2(|L|)$} & \multicolumn{2}{c}{Aurora Prover} & \multicolumn{2}{c}{Aurora Verifier} \\
			&   & GM FFT\textsuperscript{*}    &  Cantor FFT & GM FFT\textsuperscript{*}    &  Cantor FFT  \\
			&   &  \small{\cite{libiop}}   &  \small{(this work)} & \small{\cite{libiop}}    &  \small{(this work)}  \\\midrule
			10 & 17  & \ \ 0.879  & \  0.654  & 0.047 &  0.046    \\ 
			11 & 18  & \ \ 1.825  & \  1.338  & 0.063 &  0.062    \\ 
			12 & 19  & \ \ 3.755  & \  2.713  & 0.094 &  0.093    \\ 
			13 & 20  & \ \ 7.888  & \  5.727  & 0.153 &  0.151    \\ 
			14 & 21  &  \ 18.014  & \  11.708 & 0.269 &  0.264    \\ 
			15 & 22  &  \ 39.153  & \ 25.085  & 0.495 &  0.485    \\
			16 & 23  &  \ 82.302  & \ 49.981  & 0.946 &  0.926    \\
			17 & 24  &  171.490   & 102.191   & 1.885 &  1.792    \\
			18 & 25  &  363.369   & 212.064   & 3.597 &  3.506    \\
			19 & 26  &  753.485   & 435.800   & 7.100 &  6.909    \\            
			\bottomrule
		\end{tabularx}
	}
	\begin{tablenotes}
		\footnotesize
		\item \textsuperscript{*} {\scriptsize Gao-Mateer FFT using standard basis.}
	\end{tablenotes}
\end{table}


\paragraph{Contributions} Our main contributions are summarized as follows:

\begin{itemize}
	% \setlength{\itemsep}{0.8em}
	\item In the Cantor \gls{fft} algorithm, we present a theoretical analysis of the vanishing polynomials, providing a precise count of their terms based on Hamming-weight. This approach enables an accurate determination of the number of additions required. To the best of our knowledge, prior works have only reported upper bounds. We also efficiently computed the vanishing polynomials and multiplication factors, improving the efficiency even without precomputation.
	
	\item We propose building blocks for the Cantor \gls{fft} algorithm and demonstrate its significant performance improvements compared to the Gao-Mateer algorithm while using our Cantor \gls{fft}  algorithm in the current Aurora implementation \cite{libiop}. Table \ref{tab:cost_analysis_intorduction} shows how our \gls{fft}  algorithm optimizations using the Cantor special basis can accelerate the prover and verifier algorithms in Aurora.
	
	\item  For the Gao-Mateer algorithm, we also provide a detailed breakdown of its two core components: the \textsf{Expand} and \textsf{Aggregate} modules. We incorporate the Cantor special basis into the Gao-Mateer algorithm to enhance the computational and space efficiency.
	We also introduce the precomputation techniques that substantially reduce overhead in the Cantor algorithm and two levels of precomputation in the Gao-Mateer algorithm.
	
	\item We propose an analysis of the \gls{fft} call complexity in the Aurora \gls{zksnark}, evaluating the number and size of each FFT/IFFT call based on the number of constraints and variables in a given R1CS, and the target security parameter. Additionally, we demonstrate how the careful selection of the shift element in the affine subspaces of Aurora enables a significant reduction in the space complexity of the precomputation in the Cantor FFT by leveraging the unique properties of the Cantor special basis.
	
	\item We provide a \CC  implementations of the Cantor algorithm, the Gao-Mateer algorithm which uses the Cantor special basis and, our precomputation techniques. We then provide a comprehensive comparison of these algorithms, along with their respective precomputations in Figure \ref{fig:benchmark}. 
	
	
	% We adopt the Cantor special basis and our optimized FFT algorithm in the current Aurora implementation \cite{libiop}, and our optimized FFT algorithm implementation uses the same field arithmetization library (libiop \cite{libiop}) as Aurora. This ensures compatibility with the library and facilitates a fair comparison between the algorithms.
	% \item We discuss two levels of precomputation in the Gao-Mateer algorithm and provide a comprehensive comparison of the Cantor and Gao-Mateer algorithms, along with their respective optimizations.
\end{itemize}



 The chapter is structured as follows.  Section~\ref{Sec:Cantor Implementation} presents optimizations for the Cantor algorithm, while Section~\ref{Sec:Gao-implementation} details the Gao-Mateer algorithm and its precomputations. Section~\ref{Sec:FFT_Calls_in_Aurora} analyzes FFT call complexity in Aurora, and Section~\ref{Sec:ComplexityAnalysis} compares the FFT algorithms. Finally, Section~\ref{Sec:Summary} summarizes findings.


\section{Cantor Algorithm Building Blocks}\label{Sec:Cantor Implementation}

In our implementation of the Cantor algorithm, we opted for iteration rather than recursion. The recursive approach, as presented in Algorithm \ref{Algo:Cantor}, introduces extra overhead due to function calls and the increased memory needed to track these calls. Therefore, our Cantor \gls{fft} of length $n = 2^m$ consists of $m$ iterative rounds to evaluate a polynomial $f(x)\in\mathbb{F}_{2^k}[x]$ of degree $<2^m$ over the affine subspace $\theta + W_m$, where $\theta \in \mathbb{F}_{2^k}$ and $W_m$ must be generated by the Cantor special basis as described in Section \ref{sec: Preliminaries - Cantor Algorithm}. Also, for the Cantor special basis, we know that $\mathbb{Z}_{W_{i}}(x)=S^{i}(x) \text{ for } i=0,1,\ldots,m$. Thus, the coefficients of the vanishing polynomials $\mathbb{Z}_{W_{i}}(x)$ are in $\mathbb{F}_2$. Additionally, we have $S^{i}(\beta_{i}) = 1$. We can derive the following result regarding th existance of the Cantor special basis.

\begin{theorem}
	If the polynomial $S^{i}(x)$ has no solution $\beta_{i} \in \mathbb{F}_{2^k}$ such that $S^i(\beta_i) = 1$, then for any $\ell > i$, the polynomial $S^{\ell}(x)$ also has no solution for $S^{\ell}(x) = 1$ in $\mathbb{F}_{2^k}$. In other words, there does not exist a set of Cantor special basis ${\beta_0=1, \beta_1, \ldots, \beta_{\ell}}$ such that $S(\beta_j) = \beta_{j-1}$ for $j=1,\ldots,\ell$.
 \end{theorem}


Let $0 \leq r \leq m-1$ be the round number. In each round $r$, the algorithm processes $2^r$ polynomials of degree $< 2^{m-r}$, resulting in $2^{r+1}$ polynomials of degree $< 2^{m-r-1}$ at the end of the round. $f(x)$ is represented by $\fbu$, a vector of size $2^m$ that stores the coefficients as described in Section \ref{sec:vec_rep}. We use the same vector to store all intermediate polynomials during each round. For example, in round $r$, $\fbu$ stores the concatenation of  the $2^{r+1}$ polynomials. Finally, in round $r = m-1$, the algorithm outputs $2^m$ constant values stored in the vector $\fbu$, representing the evaluations of $f(x)$ over $\theta + W_m$. 
% Figure \ref{fig:Canopy} illustrates our iterative approach to implementing the Cantor algorithm.

Before presenting the details of our Cantor algorithm implementation, we first explain the selection process of the Cantor basis of length $m$, denoted as $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$. From \cite[Appendix]{Gao2010FFT}, we know that $W_m = \langle \beta_0, \beta_1, \ldots, \beta_{m-1} \rangle$ must be a subspace of the field (or subfield) $\mathbb{F}_{2^{2^\ell}}$. To determine the Cantor special basis for $\mathbb{F}_{2^{2^\ell}}$, we begin by defining a basis $\{\beta_0, \beta_1, \ldots, \beta_{2^\ell-1}\}$ such that $\text{Tr}_{\mathbb{F}_{2^{2^\ell}}/\mathbb{F}_2}(\beta_{2^\ell-1}) = 1$. Then, we recursively determine the remaining basis elements by $\beta_{j-1} = \beta_j^2 + \beta_j$ for $1 \leq j \leq 2^\ell-1$. We then select the first $m$ elements from this basis to construct our Cantor special basis of dimension $m$. In our implementation, we randomly try different values of $\beta_{2^\ell-1}$ and compute its trace. With a probability of $0.5$, this value will have a trace of $1$.


\subsection{Vanishing Polynomials}
\label{sec:Vanishing Polynomials}
In the Cantor additive FFT of length $2^m$, the vanishing polynomials $\splitatcommas{\mathbb{Z}_{W_{0}}, \mathbb{Z}_{W_{1}}, \ldots, \mathbb{Z}_{W_{m-1}}}$ must be computed to perform the division algorithm. The coefficients in each $\mathbb{Z}_{W_{i}}(x)$ are derived from
\[
\mathbb{Z}_{W_{i}}(x) = \sum_{j=0}^{i} \left[{i \choose j} \mod 2\right] x^{2^j}.
\]
Employing Lucas's theorem \cite{Lucas1878}, we efficiently compute
$
{i \choose j} \equiv \prod_{k=0}^{t-1} {i_k \choose j_k} \bmod 2,
$
where 
\[
\begin{aligned}
	i = i_0 + i_12 + i_22^2 + \ldots + i_{t-1}2^{t-1} \quad &(i_k \in \mathbb{F}_2), \\
	j = j_0 + j_12 + j_22^2 + \ldots + j_{t-1}2^{t-1} \quad &(j_k \in \mathbb{F}_2).\\
\end{aligned}
\]

\begin{theorem}\cite[Theorem 2]{Fine1947}
	The number of integers $j$ not exceeding $i$ for which $\binom{i}{j} \not\equiv 0 \pmod{2}$ is $\prod_{k=0}^{t} (i_k + 1)$.
\end{theorem}

Thus, by the above theorem we conclude that the number of non-zero coefficients in $\mathbb{Z}_{W_{i}}(x)$ equals to $2^{\text{wt}(i)}$, where $\text{wt}(i)$ denotes the Hamming weight  of $i$, i.e., the number of bits equal to $1$. Since the number of non-zero coefficients in most vanishing polynomials is considerably less than its degree (i.e., $2^{i}$), we avoid using vectorial representation of univariate polynomials described in Section \ref{sec:vec_rep} for these polynomials. Otherwise the polynomial division algorithm would require excessive addition operation on zero coefficients.

To efficiently represent the vanishing polynomials, we store the index number of the coefficients in the reverse order. Specifically, let $\zbu_i$ represent $\mathbb{Z}_{W_{i}}(x) = \sum_{j=0}^{i} c_j x^{2^j}$ in our implementation. We define
\[
\zbu_i = (\zeta_0, \zeta_1, \ldots, \zeta_{2^{\text{wt}(i)}-1}) = (2^i-2^j | c_j=1\ \text{in}\ \mathbb{Z}_{W_{i}}(x)),
\]
where Algorithm \ref{Algo:Vanishing Polynomial} describes how $\zbu_i$ is computed.

\begin{algorithm}[h]
	\caption{Vanishing Polynomial ($i$, $\theta$)} \label{Algo:Vanishing Polynomial}
	\begin{algorithmic}[1]
	
		\Require{$i \in \mathbb{N}$ and $\theta \in \mathbb{F}_{2^k}$}
		\Ensure{$\zbu_i$, $\mathsf{eval}$.}
		% $\zeta_0 \leftarrow 2^i - 1$ \\
		\State $\ell \gets 0$
		\State $\mathsf{eval} \gets \theta$
		\For{$j = 0$ to $i-1$} 
			% $\textsf{is\_one} \leftarrow \true $\\
			\State $t \gets \lceil \log_2(j+1) \rceil$  
			\State	$k \gets 0$
			\While{$(i_k  \lor  \neg j_k) \land (k < t)$}
				% $\textsf{is\_one} \leftarrow (i_k)  \lor  (\neg j_k)$ \\
				\State $k \gets k + 1$
			\EndWhile
			\If{$k=t$}
				\State $\zeta_\ell \gets 2^i - 2^j$
				\State $\mathsf{eval} \gets \mathsf{eval} + \theta^{2^j}$
				\State $\ell \gets \ell + 1$ 
			\EndIf
		\EndFor
		\State $\mathsf{eval} \gets \mathsf{eval} + \theta^{2^i}$ \Comment{ As the highest degree term is not in $\zbu_i$}
		\State \textbf{assert} $\ell = 2^{\text{wt}(i)}-2$ 
		\State \Return $\zbu_i \gets (\zeta_0, \zeta_1, \ldots, \zeta_{2^{\text{wt}(i)}-2})$, $\mathsf{eval}$.
	\end{algorithmic}
\end{algorithm}

Algorithm \ref{Algo:Vanishing Polynomial} also computes the evaluation of $\mathbb{Z}_{W_{i}}(\theta)$ which is used later in Section \ref{sec:Canopy}. Reversing the order (i.e., measuring the distance from $2^i$ rather than from $0$) eliminates the need for degree shifting in $\mathbb{Z}_{W_{i}}(x)$ during the division rounds. Additionally, since ${i \choose i} = 1$, $c_i$ is always one, and $\zeta_{2^{\text{wt}(i)}-1}$ is zero which can be omitted from $\zbu_i$. This omission excludes this coefficient from the polynomial division algorithm, saving one addition per division round while keeping the quotient in the same vector as the dividend. 
% The following section presents an efficient algorithm for dividing a polynomial by a vanishing polynomial.


\subsection{Polynomial Division}
In round $r$ of the Cantor algorithm, the $2^r$ polynomials $f_{i,r}(x)$, each of degree $< 2^{p}$, are divided by $\mathbb{Z}_{W{p-1}}(x + \theta_{i,r})$ and $\mathbb{Z}_{W{p-1}}(x + \theta_{i,r} + \beta_{p-1})$, where $p = m - r$ and $0 \leq i \leq 2^r - 1$. The corresponding remainders of these divisions for each $f_{i,r}(x)$ are outputted to be processed in the next round. Since the vanishing polynomials are linearized polynomials, we have
\begin{align*}
	\mathbb{Z}_{W{p-1}}(x + \theta_{i,r}) &= \mathbb{Z}_{W{p-1}}(x) + \mathbb{Z}_{W{p-1}}(\theta_{i,r}),\ \text{and}\\
	\mathbb{Z}_{W{p-1}}(x + \theta_{i,r} + \beta_{p-1}) &= \mathbb{Z}_{W{p-1}}(x) + \mathbb{Z}_{W{p-1}}(\theta_{i,r}) + \mathbb{Z}_{W{p-1}}(\beta_{p-1}),
\end{align*}
where $\mathbb{Z}_{W{p-1}}(\beta_{p-1}) = 1$. Since $\deg(f_i(x)) < 2\deg(\mathbb{Z}_{W{p-1}}(x))$, we reduce the two divisions required for each $f_i(x)$ to a single division by $\mathbb{Z}_{W{p-1}}(x)$, and then compute the remainders of the original divisions accordingly.

 \begin{proposition}\label{proposition:polynomial_division}
	     Let \( f(x) \) and \( g(x) \) be polynomials such that \( \deg(f) < 2^m \) and \( \deg(g) = 2^{m-1} \). Suppose that dividing \( f(x) \) by \( g(x) \) yields the quotient \( q(x) \) and the remainder \( r(x) \), so that
	     $
	     f(x) = g(x) q(x) + r(x).
	     $
	     Let \( c \) be a constant, and let \( q'(x) \) and \( r'(x) \) be the quotient and remainder when dividing \( f(x) \) by \( g(x) + c \). Then,
	     $
	     q'(x) = q(x), \quad \text{and} \quad r'(x) = r(x) - c\, q(x). 
	     $
	 \end{proposition}
\begin{proof}
	We prove this in two steps:
	\begin{enumerate}
		\item According to polynomial division algorithm, \(\deg(q) = \deg(q') = deg(f) - deg(g) < 2^{m-1}\),
		\item Let  $f(x) = (g(x)+c) q'(x) + r'(x)$, then
		\begin{align*}
			(g(x)+c) q'(x) + r'(x) &= g(x) q(x) + r(x)\\
			\implies g(x)(q'(x) - q(x)) &= r(x) - r'(x) - cq'(x).
		\end{align*}
	\end{enumerate}     
	Now, assume the opposite, that is, \( q'(x) \neq q(x) \). This implies \( q'(x) - q(x) \neq 0 \), meaning \( \deg(q'(x) - q(x)) \geq 0 \). Since \( g(x) \) divides \( g(x)(q'(x) - q(x)) \), the degree of the right-hand side \( r(x) - r'(x) - c q'(x) \) must be at least \( \deg(g) \).
	However, by the polynomial division algorithm, \( \deg(r) < \deg(g) \) and \( \deg(r') < \deg(g) \), and according to step 1, \( \deg(q') < \deg(g) = 2^{m-1} \). Therefore, \( \deg(r(x) - r'(x) - c q'(x)) < \deg(g) \), which is a contradiction.
	Hence, \( q'(x) = q(x) \), and consequently, \( r'(x) = r(x) - c\, q(x) \).
\end{proof}


Since the coefficients in $\mathbb{Z}_{W_{p-1}}(x)$ are in $\mathbb{F}_2$, division by $\mathbb{Z}_{W_{p-1}}(x)$ requires only additions. In the division algorithm, the dividend polynomial, denoted as $f_{i,r}(x)$, is added to scaled degree-shifts of $\mathbb{Z}_{W_{p-1}}(x)$. However, we eliminate the need for degree shifts by using the reversed index order in $\zbu_i$, which represents the distance of each non-zero coefficient from the highest degree (i.e., $2^p$). 

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.75\linewidth]{Figures/Division.jpg}
	\caption{Computing the polynomials for the next round from the outputs of dividing \( f_{i,r}(x) \) (of degree \( < 8 \)) by \( \mathbb{Z}_{W_{p-1}} \), represented by \( \zbu_{p-1} \). %This figure separates the computation of \( f_{2i,r+1}(x) \) from the division algorithm, despite the actual implementation.
	}  
	\label{fig:division}
\end{figure}

Dividing \( f_{i,r}(x) \), a polynomial of degree $< 2^p$, by \( \mathbb{Z}_{W_{p-1}}(x) \) yields the quotient \( q_{i,r}(x) \) and the remainder \( r_{i,r}(x) \), each with degree $< 2^{p-1}$. Let $\fbu_{i,r}$ represent the $(i+1)$-th sub-vector of $2^p$ elements in the vector $\fbu$ at the beginning of round $r$. This sub-vector stores the coefficients of \( f_{i,r}(x) \), ordered from the constant term to the highest degree term. Our polynomial division algorithm begins with the coefficient of the highest degree term in $\fbu_{i,r}$ and subtracts that coefficient from the lower-degree coefficients, spaced by distances determined by $\zbu_i$. Since $\zbu_i$ does not include zero, the coefficient of the highest degree term remains unaffected. In the next round of the polynomial division, the algorithm repeats this process with the second highest degree term. After $2^{p-1}$ rounds, the higher-degree (right) half of $\fbu_{i,r}$ stores the coefficients of \( q_{i,r}(x) \), while the lower-degree (left) half stores the coefficients of \( r_{i,r}(x) \). 
Then, given Proposition \ref{proposition:polynomial_division}, the algorithm processes the outputs of the polynomial division, which serve as inputs to the next round, as follows:
\begin{align*}
	f_{2i,r+1}(x) &= r_{i,r}(x) - \mathbb{Z}_{W_{p-1}}(\theta_{i,r})\, q_{i,r}(x),\ \text{and}\\
	f_{2i+1,r+1}(x) &= f_{2i,r+1}(x) - q_{i,r}(x),
\end{align*}
where $\mathbb{Z}_{W_{p-1}}(\theta_{i,r})$ denotes the evaluation of the vanishing polynomial at $\theta_{i,r}$. Figure \ref{fig:division} illustrates the computation of the polynomials for the next round from the quotient and remainder in each round. Our polynomial division algorithm implementation integrates the computation of the quotient, remainder, and the polynomials for the next round. 
% This integration is facilitated by the feasibility of computing the coefficients of \( f_{2i,r+1}(x) \) while determining the coefficients of \( q(x) \) sequentially, from the highest degree term to the constant term. However, the computation of \( f_{2i+1,r+1}(x) \) cannot be integrated into the division algorithm because the coefficients in \( r(x) \) are not computed sequentially; rather, they are updated selectively throughout the rounds of the algorithm. The algorithm is detailed in Algorithm \ref{Algo:Polynomial Division}.




\begin{algorithm}[h]
	\caption{Polynomial Division ($\fbu_\text{in}$, $\zbu_{p-1}$, $\mathbb{Z}_{W_{p-1}}(\theta_{i,r})$, $p$, $i$)}
	\label{Algo:Polynomial Division}
	\begin{algorithmic}[1]
		\Require $\fbu_\text{in} = (c_0,c_1,\ldots,c_{n-1})$, 
		$\zbu_{p-1} = (\zeta_0, \zeta_1, \ldots, \zeta_{2^{\text{wt}(p-1)}-2})$, 
		$\mathbb{Z}_{W_{p-1}}(\theta_{i,r}) \in \mathbb{F}_{2^k}$, 
		$p = m-r$, and $i$ determines the polynomial $f_{i,r}(x)$, of degree $<2^p$, in $\fbu_\text{in}$.
		\Ensure $\fbu_\text{out}$
		
		\State $\mathsf{offset} \gets i \times 2^p$ \Comment{The offset at which the coefficients of $f_{i,r}(x)$ are in $\fbu_\text{in}$}
		
		\For{$k = 2^{p} + \mathsf{offset} - 1$ to $2^{p-1} + \mathsf{offset}$} \Comment{Iterates over the higher-degree half in decreasing order}
		\For{$\ell = 0$ to $2^{\text{wt}(p-1)}-2$}
		\State $c_{(k-\zeta_\ell)} \gets c_{(k-\zeta_\ell)} + c_k$
		\EndFor
		\State $c_{(k-2^{p-1})} \gets c_{(k-2^{p-1})} + c_k \times \mathbb{Z}_{W_{p-1}}(\theta_{i,r})$ \Comment{Computes $f_{2i,r+1}(x)$} \label{Step:CantorLeftSib}
		\EndFor
		
		\For{$k = 2^{p-1} + \mathsf{offset}$ to $2^{p} + \mathsf{offset} - 1$}
		\State $c_k \gets c_k + c_{(k - 2^{p-1})}$ \Comment{Computes $f_{2i+1,r+1}(x)$} \label{Step:CantorRightSib}
		\EndFor
		
		\State \Return $\fbu_\text{out} \gets (c_0,c_1,\ldots,c_{n-1})$
	\end{algorithmic}
\end{algorithm}

The polynomial division algorithm requires \( 2^{p-1} \times \left(2^{\text{wt}({p-1})} + 1\right) \) additions and \( 2^{p-1} \) multiplications. The peak space complexity is \( O(1) \) since the algorithm does not require any auxiliary vectors for storing intermediate values. 
% The algorithm gets $\fbu_\text{in}$, the vector formed by concatenating the $2^r$ vectors, each of size $2^{p}$, representing the coefficients in the polynomials $f_{i,r}(x)$ of degree $<2^p$, where $p = m - r$. The input \( p \) specifies the degree of the polynomial, while the input \( i \) represents the index of the polynomial in the current round (\( r \)), where \( 0 \leq i < 2^r - 1 \).


% The vector $\zbu_{p-1}$, representing $\mathbb{Z}_{W_{p-1}}$ in the format described in Section \ref{sec:Vanishing Polynomials}, and the constant $\mathbb{Z}_{W_{p-1}}(\theta_{i,r}) \in \mathbb{F}_{2^k}$ are two additional inputs to Algorithm \ref{Algo:Polynomial Division}. Their values are associated with $m$, $r$, and the index of the polynomial in that round, $i$. 
The Canopy module, described in the next section, is responsible for providing all of the inputs of the polynomial division algorithm.





\subsection{Canopy Module} \label{sec:Canopy}
% The only repetitive operation of the Cantor algorithm is transforming an input polynomial of degree \( < 2^p \) into two polynomials, each with a degree \( < 2^{p-1} \), where $p=m-r$ in round $r$. This operation is repeated on the resulting polynomials over multiple rounds until the degree of the output polynomial becomes zero. In the previous section, we described how a polynomial should be transformed into two polynomials required for the next round via our polynomial division algorithm (Algorithm \ref{Algo:Polynomial Division}). However, the inputs for that algorithm will be determined by the module presented in this section.
% \textit{Canopy} is the module for performing the primary operation of the Cantor algorithm. 
% The module name is chosen to reflect both its function as the Cantor Operator (CanOP) and its hierarchical structure, where each Canopy module layers over its successor. 
Cantor algorithm is implemented by Canopy modules of varying input sizes $2^p$ and indices $i$, denoted as $\mathsf{Canopy}_{p, i}$. The index $i$ indicates the module number in each round and determines the offset from the start of the $\fbu$ vector, where the coefficients of the input polynomial begin with $\mathsf{offset} = i2^p$. Figure \ref{fig:Canopy} illustrates our iterative approach of using Canopy modules to implementing the Cantor algorithm.


 \begin{figure}
	     \centering
	     \includegraphics[width=\linewidth]{Figures/Canopy.jpg}
	     \caption{\small Cantor FFT algorithm of length $n = 2^3$ which evaluates $f(x)\in\mathbb{F}[x]$ of degree $<8$ over $\theta$ + $W_3$, where $\theta \in \mathbb{F}_{2^k}$ and $W_3$ is the Cantor special basis.}
	     \label{fig:Canopy}
	 \end{figure}%\vspace{-100pt}

The $\mathsf{Canopy}_{p, i}$ determines $\theta_{i, r}$ and then evaluate $\mathbb{Z}_{W_{p-1}}(\theta_{i, r})$, which is necessary for running Algorithm~\ref{Algo:Polynomial Division}.  Let Cantor algorithm evaluate  $f(x)$ of degree $< 2^m$ over $\theta + \langle \beta_0 = 1, \beta_1, \ldots, \beta_{m-1} \rangle$.  
% Here, we describe how to efficiently determine $\theta_{i, r}$ for each $\mathsf{Canopy}_{p, i}$. 
Let $\theta_{0, 0} = \theta$, and for $1 \leq r \leq m-1$ and $0 \leq i \leq 2^r - 1$, $\theta_{i, r}$ is determined recursively according to the following rules:

\begin{equation*}
	\theta_{i, r} =
	\begin{cases}
		\theta_{i / 2, r - 1} & \text{if } i \bmod 2 = 0,\\
		\theta_{(i - 1) / 2, r - 1} + \beta_{p} & \text{if } i \bmod 2 = 1,
	\end{cases}
\end{equation*}
% \begin{align*}
	%     \theta_{2i, r+1} &= \theta_{i, r}, \\
	%     \theta_{2i + 1, r+1} &= \theta_{i, r} + \beta_{m-r}.
	% \end{align*}
where $p=m-r$. This equation can be simplified to 
$
\theta_{i, r} = \theta_{\lfloor i / 2 \rfloor, r - 1} + (i \bmod{2}) \beta_{p},
$
and can be written as,
\begin{equation*}
	\theta_{i, r} = \theta + \sum_{j=0}^{r - 1} \left( \lfloor \frac{i}{2^j} \rfloor \bmod{2} \right) \beta_{p+j} = \theta + \sum_{j=0}^{r - 1} i_j \, \beta_{p+j},
\end{equation*}
where $i = i_0 + i_12 + i_22^2 + \ldots + i_{r-1}2^{r-1}$ ($i_j \in \mathbb{F}_2$), denotes the binary representation of $i$. Then, to evaluate $\mathbb{Z}_{W_{p-1}}(\theta_{i, r})$, we employ the rule $S^{i}(\beta_{i+\ell}) = \beta_{\ell}$ provided earlier in this section to simplify the computation. Specifically, we write $\mathbb{Z}_{W_{p-1}}(\beta_{p-1 + j}) = \beta_j$. Therefore $\mathbb{Z}_{W_{p-1}}(\theta_{i, r})$ can be evaluated as
\begin{equation*}
	\setlength{\jot}{3pt}  % Reduces space between lines in multi-line equations
	\mathbb{Z}_{W_{p-1}}(\theta_{i, r}) =  \mathbb{Z}_{W_{p-1}}(\theta) + \sum_{j=0}^{r - 1} i_j \, \beta_{j+1},
\end{equation*}
where \(\mathbb{Z}_{W_{p-1}}(\theta)\) is evaluated at each round while constructing \(\zbu_{p-1}\) from \(\mathbb{Z}_{W_{p-1}}(x)\) during Algorithm~\ref{Algo:Vanishing Polynomial}. The computation of \(\mathbb{Z}_{W_{p-1}}(\theta)\) is shared across all the \(\mathsf{Canopy}\) modules in each row since the vanishing polynomial remains consistent. 

\begin{algorithm}[h]
	\caption{$\mathsf{Canopy}_{p, i}$ ($\fbu_\text{in}$, $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$, $\zbu_{p-1}$, $\mathbb{Z}_{W_{p-1}}(\theta)$, $p$, $i$)}
	\label{Algo:Canopy}
	\begin{algorithmic}[1]
		\Require $\fbu_\text{in}$, $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$ is the Cantor special basis, 
		$\zbu_{p-1} = (\zeta_0, \zeta_1, \ldots, \zeta_{2^{\text{wt}(i)}-2})$, 
		$\mathbb{Z}_{W_{p-1}}(\theta) \in \mathbb{F}_{2^k}$, 
		$p = m-r$, and $i = (i_0, i_1, i_2, \ldots, i_r)$.
		\Ensure $\fbu_\text{out}$
		
		\State $\psi_{i, r} \gets \mathbb{Z}_{W_{p-1}}(\theta)$
		
		\For{$j = 0$ to $r-1$}
		\State $\psi_{i, r} \gets \psi_{i, r} + i_j \times \beta_{j+1}$ \label{line:canopy-psi}
		\EndFor
		
		\State $\fbu_\text{out} \gets$ \Call{Polynomial Division}{$\fbu_\text{in}$, $\zbu_{p-1}$, $\psi_{i, r}$, $p$, $i$} \Comment{Algorithm \ref{Algo:Polynomial Division}, where $\mathbb{Z}_{W_{p-1}}(\theta_{i,r}) = \psi_{i, r}$}  
		
		\State \Return $\fbu_\text{out}$
	\end{algorithmic}
\end{algorithm}


\begin{algorithm}[h]
	\caption{Cantor Algorithm ($\fbu_\text{in}$, $\theta$, $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$)}
	\label{Algo:Cantor_Implementation}
	\begin{algorithmic}[1]
		\Require $\fbu_\text{in}$ is a vector of size $2^m$ representing the coefficients in $f(x)$ (where $\deg{f} < 2^m$), 
		$\theta \in \mathbb{F}_{2^k}$ is the affine shift, 
		and $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$ is the Cantor special basis.
		\Ensure $\fbu_\text{out}$ is the vector of evaluations of $f(x)$ over $\theta + \langle \beta_0, \beta_1, \ldots, \beta_{m-1} \rangle$.
		
		\For{$r = 0$ to $m-1$}
		\State $p \gets m - r$
		\State $\zbu_{p-1}, \mathsf{eval} \gets$ \Call{Vanishing Polynomial}{$p-1$, $\theta$} \Comment{Algorithm \ref{Algo:Vanishing Polynomial}} \label{line:cantor-vanishing}
		
		\For{$i = 0$ to $2^r - 1$}
		\State \Call{$\mathsf{Canopy}_{p, i}$}{$\fbu_\text{in}$, $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$, $\zbu_{p-1}$, $\mathsf{eval}$, $p$, $i$} \Comment{Algorithm \ref{Algo:Canopy}}
		\EndFor
		\EndFor
		
		\State \Return $\fbu_\text{out}$
	\end{algorithmic}
\end{algorithm}


Algorithm~\ref{Algo:Canopy} details the steps within the \(\mathsf{Canopy}_{p, i}\) module, and Algorithm~\ref{Algo:Cantor_Implementation} describes the implementation of the Cantor algorithm based on those modules. 

\subsection{Detailed Cost Analysis}
At the $r$th iteration of the algorithm, we perform $2^{r}$ divisions by the polynomial $\mathbb{Z}_{W_{m-r-1}}(x)$. Consequently, the total number of additions resulting from polynomial division in the Cantor additive FFT is given by

\begin{equation*}
	\begin{aligned}
		\displaystyle{\sum_{r=0}^{m-1} 2^r \cdot 2^{m-r-1} \big( 2^{\text{wt}(m-r-1)}-1 \big) }
		= 2^{m-1} \displaystyle{\sum_{r=0}^{m-1} 2^{\text{wt}(r)} } - m2^{m-1}
	\end{aligned}
\end{equation*}

From Steps~\ref{Step:CantorLeftSib} and \ref{Step:CantorRightSib} of Algorithm~\ref{Algo:Polynomial Division}, we know that for the input polynomial in the $(r+1)$th iteration, we require $2 \times 2^r$ polynomial additions, each of degree less than $2^{m-r-1}$. This leads to a total of  $\sum_{r=0}^{m-1} 2\cdot 2^{r} \cdot 2^{m-r-1}=2^m m$ additions. Therefore, the total number of additions in the Cantor additive FFT is given by \[2^m m + 2^{m-1} \displaystyle{\sum_{r=0}^{m-1} 2^{\text{wt}(r)} } - m2^{m-1}= \frac{1}{2} n\log_2 n + \frac{1}{2}n \sum_{r=0}^{\log_2(n)-1} 2^{\text{wt}(r)}.\]

% \begin{equation*}
	%     \begin{aligned}
		%         &\displaystyle{\sum_{r=0}^{m-1} 2\cdot 2^{r} \cdot 2^{m-r-1} } + 2^{m-1} \displaystyle{\sum_{r=0}^{m-1} 2^{\text{wt}(r)} } - m2^{m-1}\\
		%         &= \frac{1}{2} 2^m m + \frac{1}{2} 2^m \displaystyle{\sum_{r=0}^{m-1} 2^{\text{wt}(r)} }= \frac{1}{2} n\log_2 n + \frac{1}{2}n \sum_{r=0}^{\log_2(n)-1} 2^{\text{wt}(r)}.
		%     \end{aligned}
	% \end{equation*}

This provides an exact count of the additions required in the Cantor additive FFT, whereas previous works, to the best of our knowledge, have only established upper bounds.

On the other hand, from Step~\ref{Step:CantorLeftSib} of Algorithm~\ref{Algo:Polynomial Division}, we know that for the input polynomial in the $(r+1)$th iteration, we require $2^r \times 2^{m-r-1}= 2^{m-1}$ multiplications. Thus, the number of multiplications in the Cantor additive FFT is given by
\(\sum_{r=0}^{m-1} 2^{m-1} = \frac{1}{2} n \log_2 n .\)

If the Cantor additive FFT is performed over a subspace $W_m$, due to Step~\ref{Step:CantorLeftSib} of the Algorithm~\ref{Algo:Polynomial Division}, we must account for a reduction of $\sum_{r=0}^{m-1} 2^{m-r-1}=2^m-1=n-1$ in both additions and multiplications. Thus, the costs for additions and multiplications are changed to
$\frac{1}{2} n\log_2 n + \frac{1}{2}n \sum_{r=0}^{\log_2(n)-1} 2^{\text{wt}(r)} -n +1$ and $\frac{1}{2} n \log_2 n-n+1$, respectively.
% In our instantiations of the Cantor algorithm, we can precompute some values that are independent of the input polynomial and only depend on the length of the FFT algorithm (i.e., the upper bound degree for the polynomial). Therefore, these values can be precomputed once for the Cantor algorithm of length $n$ and then used for evaluations of any polynomial whose degree is $< n = 2^m$. In the next section we suggest an algorithm for the precomputation.

\subsection{Precomputation} \label{sec:cantor_precmp}
% To avoid wasting computational power by recomputing certain values when the FFT algorithm is used to evaluate polynomials of the same degree over and over again, over the same evaluation set (i.e., $\theta + W_m$), we present our precomputation algorithm, which is executed once and stores the reused values in memory.

% In our instantiations of the Cantor algorithm, we can precompute some values that are independent of the input polynomial and only depend on the length of the FFT algorithm (i.e., the upper bound degree for the polynomial). Therefore, these values can be precomputed once for the Cantor algorithm of length $n$ and then used for evaluations of any polynomial whose degree is $< n = 2^m$. In this section, we suggest an algorithm for the precomputation.

The computations of $\mathbb{Z}_{W_{p-1}}(\theta_{i, r})$ for each \(\mathsf{Canopy}_{p, i}\), denoted as $\psi_{i, r}$ in Line \ref{line:canopy-psi} of Algorithm \ref{Algo:Canopy}, and the computation of the vanishing polynomials in Line \ref{line:cantor-vanishing} of Algorithm \ref{Algo:Cantor_Implementation}, do not depend on the input polynomial. 
The storage required to store the precomputed values for the Cantor algorithm of length $n = 2^m$ is 
$
\sum_{i=0}^{m-1} \left(2^{\text{wt}(i)} - 1\right)
$
integers to store $\mathbf{Z} = (\zbu_0, \zbu_1, \ldots, \zbu_{m-1})$ and $2^m - 1$ field elements to store $\boldsymbol{\Psi} = (\boldsymbol{\psi}_0, \boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_{m-1})$, where $\boldsymbol{\psi}_r = ( \psi_{0,r}, \psi_{1,r}, \ldots, \psi_{2^r-1,r} )$. Algorithm \ref{Algo:Cantor_Precomp}  describes the precomputation algorithm. 

\begin{algorithm}[h]
	\caption{Cantor Precomputation ($\theta$, $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$)}
	\label{Algo:Cantor_Precomp}
	\begin{algorithmic}[1]
		\Require $\theta \in \mathbb{F}_{2^k}$ is the affine shift, and $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$ is the Cantor special basis.
		\Ensure $\mathbf{Z} = (\zbu_0, \zbu_1, \ldots, \zbu_{m-1})$ is a vector of vanishing polynomials,  
		$\boldsymbol{\Psi} = (\boldsymbol{\psi}_0, \boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_{m-1})$,  
		where $\boldsymbol{\psi}_r = ( \psi_{0,r}, \psi_{1,r}, \ldots, \psi_{2^r-1,r} )$
		
		\For{$r = 0$ to $m - 1$}
		\State $\zbu_i, \mathsf{eval} \gets$ \Call{Vanishing Polynomial}{$m - r - 1$, $\theta$}
		\For{$i = 0$ to $2^r - 1$}
		\State $\psi_{i,r} \gets \mathsf{eval}$
		\For{$j = 0$ to $r - 1$}
		\State $\psi_{i,r} \gets \psi_{i,r} + i_j \times \beta_{j+1}$
		\EndFor
		\EndFor
		\State $\boldsymbol{\psi}_r \gets ( \psi_{0,r}, \psi_{1,r}, \ldots, \psi_{2^r - 1,r} )$
		\EndFor
		
		\State \Return $\mathbf{Z} \gets (\zbu_0, \zbu_1, \ldots, \zbu_{m-1}),\ \boldsymbol{\Psi} \gets (\boldsymbol{\psi}_0, \boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_{m-1})$
	\end{algorithmic}
\end{algorithm}



For a special case where the affine shift $\theta$ is an element of the Cantor special basis, we do not have to pre-compute all $\mathbb{Z}_{W_{p-1}}(\theta_{i, r})$, since these values are only combinations of the cantor basis. In this case we can precompute the lookup table where each store the combinations of subset of Cantor basis elements. 


\section{Gao-Mateer Algorithm Building Blocks}\label{Sec:Gao-implementation}

% Similar to the Cantor algorithm implementation, an iterative implementation of the Gao-Mateer algorithm is more efficient than a recursive approach, particularly in terms of memory usage and stack management. Thus, 
The Gao-Mateer FFT implementation of length $n = 2^m$ consists of $2m$ iterative rounds to evaluate a polynomial $f(x) \in \mathbb{F}_{2^k}[x]$ of degree $< 2^m$ over the affine subspace $\theta + W_m$, where $\theta \in \mathbb{F}_{2^k}$ and $W_m = \langle \beta_0, \beta_1, \ldots, \beta_{m-1} \rangle$. % such that $\beta_1, \ldots, \beta_{m-1} \in \mathbb{F}_{2^k}$ are linearly independent over $\mathbb{F}_2$.

We describe the Gao-Mateer algorithm through two primary modules: the \textsf{Expand} module and the \textsf{Aggregate} module.  \textsf{Expand} is an $r$-round algorithm where in each round $0 \leq r \leq m - 1$, it expands $2^r$ polynomials of degree $< 2^{m-r}$ into $2^{r+1}$ smaller polynomials of degree $<2^{m-r-1}$. Similar to our Cantor algorithm implementation, only one vector of length $2^m$ denoted as $\fbu$ is required to store all the polynomials in each round. \textsf{Aggregate} is an $r$-round algorithm that takes the output of \textsf{Expand}, and iteratively folds them over $r$ rounds, ultimately producing the evaluations of $f(x)$ on $\theta + W_m$. In the following, we start by presenting the Taylor expansion algorithm, which serves as the core component of \textsf{Expand}.

% Line \ref{line:gao-last_step_evaluation} of Algorithm \ref{Algo:Gao} which evaluates polynomials of degree $<2$ at $\theta_0 + \langle \beta_{m-1,0}\rangle$ is the step that \textsf{Expand} modules algorithm terminates and \textsf{Aggregate} modules starts. Such that, in the last round of the \textsf{Expand} modules, the coefficients of the degree-one terms in those polynomials are multiplied by the scaling factor denoted as $\beta_{m-1,0}$. Then in the first round of 

% The evaluating polynomials of degree $<2$, as described in Line \ref{line:gao-last_step_evaluation} of Algorithm \ref{Algo:Gao}, the coefficients of the degree-one terms in these polynomials are multiplied by the scaling factor during the final round of the \textsf{Expand} module. The remaining steps are completed in the first round of the \textsf{Aggregate} module.




\subsection{Taylor Expansion} \label{sec:Taylor Expansion}

Let $f(x) = \sum_{i=0}^{m-1}c_ix^i, c_i \in \F_{q}$, $\delta(x)=x^2+x$. We denote $y=\delta(x)$. Then  we may write $f$ as
\begin{equation}\label{eq:T1}
	f(x) = f_0(y) + x f_1(y)
\end{equation}
Here, $f_i$, $i \in \{0, 1\}$, are polynomials in $\mathbb{F}_{2^k}$ with degree $< 2^{m-1}$. The representation of $f$ by (\ref{eq:T1}) is referred to as the Taylor expansion in \cite{Gao2010FFT}. The polynomials $f_i$ can be obtained iteratively as follows. Let $\fbu = (c_0, \ldots, c_{n-1})$, referred to as the coefficient vector of $f(x)$, where $n = 2^m$, denoted as $f(x)\leftrightarrow \fbu$.   Let $t = {m-2}$, we define a \textit{quadrant concatenation} of  $\fbu$ as 
$
\fbu = (\cbu_0, \cbu_1, \cbu_2, \cbu_3)
$
where $c_i = (c_{i\cdot 2^{t}}, c_{i\cdot 2^{t} +1}, \ldots, c_{(i+1)\cdot 2^{t}-1})$.
% \[
% \begin{array}{ll}
	%      \cbu_0& =(c_0, c_1, \ldots, c_{2^{t}-1})  \\
	%      \cbu_1& =(c_{2^t}, c_{2^t+1}, \ldots, c_{2\cdot 2^{t}-1}) \\ 
	%      \cbu_2& =(c_{2\cdot 2^{t}},  c_{2\cdot 2^{t}+1}, \ldots, c_{3\cdot 2^{t}-1}) \\ 
	%        \cbu_3& =(c_{3\cdot 2^{t}}, c_{3\cdot 2^{t} +1}, \ldots, c_{4\cdot 2^{t}-1}).  \\ 
	% \end{array}
% \]
In the following, we model the Taylor expansion of $f$ in terms of the concept of $\mathsf{T}_n$ module. 
The $\mathsf{T}_n$ module operation on $f$ is defined as
$
\bbu = (\cbu_0, \cbu_1+\hbu, \hbu, \cbu_3), \mbox{where } \hbu=\cbu_2+\cbu_3.
$
From $\bbu=(\bbu_0, \bbu_1, \bbu_2, \bbu_3)$, we have the following result.
\begin{lemma}\label{le:1}
	With the above notation, we have   
	\begin{equation}\label{eq:10}
		f(x) = g_0(x) + g_1(x)y^{2^t}, \mbox{where }\, y= \delta(x)     
	\end{equation}
	where $g_0(x) \leftrightarrow (\bbu_0, \bbu_1)$ and $g_1(x) \leftrightarrow (\bbu_2, \bbu_3)$.
	% \[
	% \begin{array}{l}
		% g_0(x) \leftrightarrow (\bbu_0, \bbu_1)\\
		% g_1(x) \leftrightarrow (\bbu_2, \bbu_3).
		% \end{array}
	% \]
	
	% {\color{blue} 
		% \[
		% \begin{array}{l}
			% g_0(x) \leftrightarrow \bbu[2i]\\
			% g_1(x) \leftrightarrow \bbu[2i+1],
			% \end{array}
		% \]
		% where $i\in[0,m/2)$.
		% }
\end{lemma}


%$f(x)$ is denoted as follows:



% \subsection{Algorithm for the Taylor Expansion}

Now, to compute the Taylor expansion of $f(x) \in \mathbb{F}[x]$ of degree $<n = 2^m$, the Taylor expansion of $f(x)$ at $\delta(x)=x^2+x$, denoted as $\mathsf{TE}(f, n, 2)$ can be computed through the Taylor expansions of two polynomials:

\begin{equation}\label{eq:T2}
	\mathsf{TE}(f, n, 2) = (\mathsf{TE}(g_0, n, 2), \mathsf{TE}(g_1, n, 2)) 
\end{equation}
where $g_0$ and $g_1$ are computed in Lemma \ref{le:1} by $\mathsf{T}_n$ module. 

% \begin{algorithm}[h]\label{Algo:Cantor_Precomp}
	%     \caption{Cantor Precomputation ($\theta$, $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$)}
	%     \SetAlgoLined
	%     \KwIn{$\theta \in \mathbb{F}$ is the affine shift, and $\{\beta_0, \beta_1, \cdots, \beta_{m-1}\}$ is the Cantor special basis.}
	%     \KwOut{$\mathbf{Z} = (\zbu_0, \zbu_1, \ldots, \zbu_{m-1})$ is a vector of vanishing polynomials, $\boldsymbol{\Psi} = (\boldsymbol{\psi}_0, \boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_{m-1})$, where $\boldsymbol{\psi}_r = ( \psi_{0,r}, \psi_{1,r}, \ldots, \psi_{2^r-1,r} )$.}
	
	
	%     \For{$r=0$ \KwTo $m-1$}{https://www.mozilla.org/firefox/?utm_medium=firefox-desktop&utm_source=bookmarks-toolbar&utm_campaign=new-users&utm_content=-global
		%             $\zbu_i, \mathsf{eval} \leftarrow $ Vanishing Polynomial ($m-r-1$ , $\theta$) \\
		%         \For{$i=0$ \KwTo $2^r-1$}{
			%             $\psi_{i, r} \leftarrow \mathsf{eval}$\\
			%             \For{$j=0$ \KwTo $r-1$}{
				%                 $\psi_{i, r} \leftarrow \psi_{i, r} + i_j \times \beta_{j+1}$
				%             }
			%         }
		%         $\boldsymbol{\psi}_r \leftarrow ( \psi_{0,r}, \psi_{1,r}, \ldots, \psi_{2^r-1,r} )$ \\
		%     }
	%     \Return $\mathbf{Z} \leftarrow (\zbu_0, \zbu_1, \ldots, \zbu_{m-1}), \boldsymbol{\Psi} \leftarrow  (\boldsymbol{\psi}_0, \boldsymbol{\psi}_1, \ldots, \boldsymbol{\psi}_{m-1})$.
	% \end{algorithm}

% Define a recursive process that splits the sequence based on the least significant bit (LSB):
% \begin{itemize}
	%     \item Even-index group: Indices where the LSB is 0.
	%     \item Odd-index group: Indices where the LSB is 1.
	% \end{itemize}



% \begin{example}
	%     \[
	%     \begin{array}{c}
		%         \set{0, 1, 2, 3, 4, 5, 6, 7} \\
		%         \downarrow \text{ Step 1}\\
		%         \set{0, 2, 4, 6} \quad \set{1, 3, 5, 7} \\
		%         \downarrow \text{ Step 2}\\
		%         \set{0, 4} \quad \set{2, 6} \quad \set{1, 5} \quad \set{3, 7} \\
		%     \end{array}
	%     \]
	% \end{example}


\begin{theorem} \label{th:even_odd}
	Let $\ubu=(u_0, u_1, \ldots, u_{n-1})$ be the output at the  recursion $t$ in (\ref{eq:T2}),  then the Taylor expansion of $f$, defined by (\ref{eq:T1}), is given by  
	\[
	\begin{array}{l}
		f_0 \leftrightarrow (u_0, u_2, \ldots, u_{2i}, \ldots, u_{2\lfloor{n/2\rfloor}})\\
		f_1 \leftrightarrow (u_1, u_3, \ldots, u_{2i+1}, \ldots, u_{2\lfloor{n/2\rfloor}+1}),\\
	\end{array}
	\]
	where $f_0$ and $f_1$, are polynomials each of degree $<\lfloor{n/2\rfloor}=2^{m-1}$.
\end{theorem}

As described in the above theorem, the polynomials $f_0$ and $f_1$ are constructed by performing an even-odd rearrangement on $\ubu$. Subsequently, as described in Algorithm \ref{Algo:Gao}, the evaluations $\mathsf{TE}(f_0, n/2, 2)$ and $\mathsf{TE}(f_1, n/2, 2)$ also need to be computed and so on their outputs as well. Alternatively, as implemented in \cite{libiop}, instead of performing the even-odd rearrangements, the positions of the coefficients of these two polynomials can be retained within $\ubu$. This allows us to compute a single $\mathsf{TE}(u, n/2, 2)$ on the entire polynomial $u(x)$, represented by $\ubu$, in $t-1$ recursive steps (one fewer recursion than required for $f$). Finally, to account for the skipped even-odd rearrangements, in the last round the resulting vector is rearranged in bit-reversal order defined as follows: 

Define a recursive process that splits the sequence of indices $\set{0, 1, \ldots, 2^n - 1}$ into two groups based on the least significant bit (LSB): the even-index group, containing indices where the LSB is 0, and the odd-index group, containing indices where the LSB is 1. Recursively apply the same splitting process to each group based on the next least significant bit until all groups contain exactly two indices.
% \noindent Define a recursive process that splits the sequence of indices $\set{0, 1, \ldots, 2^n - 1} $ based on the least significant bit (LSB) as follows: 

% First, separate the indices into two groups:
%     \begin{itemize}
	%         \item Even-index group: Indices where the LSB is 0.
	%         \item Odd-index group: Indices where the LSB is 1.
	%     \end{itemize}



\begin{proposition} \label{proposition: bit reversal}
	The final arrangement of indices, after applying the recursive splitting process, represents the bit-reversal of the original sequence of indices $\set{0, 1, \ldots, 2^n - 1}$.
\end{proposition}


\begin{proof}
	We will prove this statement using mathematical induction on $n$.
	
	For $n = 2$, we have the set of indices $\set{0, 1, 2, 3}$. In binary, the indices are represented as $\set{00_2, 01_2, 10_2, 11_2}$. The recursive splitting based on the least significant bit (LSB) yields:
	\begin{itemize}
		\item Even-index group: $\set{0, 2}= \set{00_2, 10_2}$ (LSB = 0)
		\item Odd-index group: $\set{1, 3}= \set{01_2, 11_2}$ (LSB = 1)
	\end{itemize}
	
	Thus, after splitting, the order is $\set{0, 2, 1, 3}$, which corresponds to the bit-reversal of the binary representations of the original sequence:
	\[
	\set{00_2, 01_2, 10_2, 11_2} \quad \text{becomes} \quad \set{00_2, 10_2, 01_2, 11_2}.
	\]
	Hence, the base case holds for $n=2$.
	
	Now, assume that for some $n = k \geq 2$, the recursive process results in a bit-reversal permutation for the sequence of indices $\set{0, 1, \ldots, 2^k - 1}$. That is, after the recursive splitting, the sequence of indices corresponds to the bit-reversal of their binary representations.
	
	We need to show that the result holds for $n = k+1$.
	
	Consider the sequence $\set{0, 1, \ldots, 2^{k+1} - 1}$ of $2^{k+1}$ elements. This sequence can be split into two groups based on the least significant bit (LSB):
	\begin{itemize}
		\item Even-index group $E$: Indices where the LSB is 0, i.e., 
		\[
		E = \set{0, 2, 4, \ldots, 2^{k+1} - 2}.
		\]
		\item Odd-index group $O$: Indices where the LSB is 1, i.e., 
		\[
		O = \set{1, 3, 5, \ldots, 2^{k+1} - 1}.
		\]
	\end{itemize}
	
	Note that each group $E$ and $O$ contains $2^k$ indices. By the inductive hypothesis, the recursive splitting within each group $E$ and $O$ will result in the bit-reversal of the indices within that group.
	
	After recursively applying the bit-reversal process to both groups, the final sequence of indices is formed by first placing the indices from the even group $E$ (which are already in bit-reversed order) followed by the indices from the odd group $O$ (also in bit-reversed order).
	\begin{itemize}
		\item Indices in $E$ correspond to even indices from the original sequence $\set{0, 1, \ldots, 2^{k+1} - 1}$.
		\item Indices in $O$ correspond to odd indices from the original sequence $\set{0, 1, \ldots, 2^{k+1} - 1}$.
	\end{itemize}
	
	Note that the binary representations of the indices in $E$ all end in $0$, and those in $O$ all end in $1$. When these groups are combined back together, the resulting sequence is formed by first appending the bit-reversed even-index group $E$, followed by the bit-reversed odd-index group $O$.
	
	Since the even-index group $E$ and odd-index group $O$ are already in bit-reversed order, combining them results in the bit-reversal of the entire sequence $\set{0, 1, \ldots, 2^{k+1} - 1}$.
	
	Hence, by the principle of mathematical induction, the result holds for all $n$.
	
	% \[
	% \begin{array}{c}
		%     S_0 = \set{0, 1, 2, \ldots, 2^{n}-3, 2^{n}-2, 2^{n}-1} \\
		%     \downarrow \text{ Step 1} \\
		%     S_{1,0}= \set{0, 2, \ldots, 2^{n}-2} = \set{i \in S_0 \mid i \equiv 0 \, (\text{mod } 2)} \quad S_{1,1}=\set{1, 3, \ldots, 2^{n}-1}= \set{i \in S_0 \mid i \equiv 1 \, (\text{mod } 2)} \\
		%     \downarrow \text{ Step 2}\\
		%     S_{2,0} =\set{i \in S_{1,0} \mid i \equiv 0 \, (\text{mod } 2^2)} \quad S_{2,1} =\set{i \in S_{1,0} \mid i \equiv 2 \, (\text{mod } 2^2)} \quad S_{2,2} =\set{i \in S_{1,1} \mid i \equiv 1 \, (\text{mod } 2^2)} \quad S_{2,3} =\set{i \in S_{1,1} \mid i \equiv 3 \, (\text{mod } 2^2)} \\
		%     \downarrow \\
		%     \vdots \\
		%     \set{0, 2^i, \ldots, 2^n-2^i} \quad 
		% \end{array}
	% \]
\end{proof}



Figure \ref{fig:gao-expand} compares the two approaches for computing the Taylor expansion in each round of the \textsf{Expand} module, as described in the next section. We present the alternative Taylor Expansion algorithm using an iterative approach in Algorithm \ref{Algo:TaylorExpansion}, where the input parameter $r$ specifies how many iterations are skipped from the end of the algorithm. Specifically, the algorithm runs for $m - r - 1$ iterations for the input length of $2^m$. Algorithm \ref{Algo:TaylorExpansion}, on input of size $n = 2^m$ and parameter $r$, involves $2^{m-1}(m - r - 1)$ finite field additions. In the next section, we present our \textsf{Expand} module where we employed an alternative approach, as implemented in \cite{libiop}.











\begin{algorithm}[h]
	\caption{Taylor Expansion ($\fbu_\text{in}, r$)}
	\label{Algo:TaylorExpansion}
	\begin{algorithmic}[1]
		\Require $\fbu_\text{in} = (c_0,c_1,\ldots,c_{n-1})$ and $r$ denotes the number of rounds reduction.
		\Ensure $\fbu_\text{out}$
		
		\State $k \gets m - 2$
		
		\While{$k \ge r$}
		\State $j \gets 0$
		
		\While{$j \leq n - 4\cdot2^k$}
		\Comment{The following for-loop implements $\mathsf{T}_{4\cdot2^k}$}
		\For{$i = 0$ to $2^k - 1$}
		\State $c_{2\cdot2^k + i + j} \gets c_{2\cdot2^k + i + j} + c_{3\cdot2^k + i + j}$
		\State $c_{2^k + i + j} \gets c_{2^k + i + j} + c_{2\cdot2^k + i + j}$
		\EndFor
		\State $j \gets j + 4\cdot2^k$ \Comment{Sets the offset for the starting indices of $\mathsf{T}_{4\cdot2^k}$ modules}
		\EndWhile
		
		\State $k \gets k - 1$
		\EndWhile
		
		\State \Return $\fbu_\text{out} \gets (c_0,c_1,\ldots,c_{n-1})$
	\end{algorithmic}
\end{algorithm}


\subsection{Expand Module}

The \textsf{Expand} module involves multiple invocations of Algorithm \ref{Algo:TaylorExpansion}, polynomial scaling (e.g., $f(\beta_m x)$), and computing basis vectors and shifts for the \textsf{Aggregate} module. 
As discussed in Section \ref{sec:Taylor Expansion}, we omit even-odd rearrangements, 
% which alter the order of the coefficients in the polynomials embedded in $\fbu$, 
so the coefficients of terms with the same degree are placed next to each other. This allows \textsf{Expand} to multiply consecutive elements by the same scaling factor. 

In the last round of the \textsf{Expand} module, $\fbu$ is rearranged in bit-reversal order. Let the bit-reversed index of $j$ be denoted as $j\text{-}\mathsf{rev}$. During the bit-reversal rearrangement process, the elements \( c_j \) where \( j = j\text{-}\mathsf{rev} \) are not swapped. These elements are referred to as fixed elements. For an input of length \( n = 2^m \), the number of fixed elements is \( 2^{\lceil m/2 \rceil} \). Consequently, the algorithm requires \( 6 \times \frac{2^m - 2^{\lceil m/2 \rceil}}{2} \) memory accesses, as each swap involves 3 memory reads and 3 memory writes. Algorithm \ref{Algo:Expand} details the \textsf{Expand} module. The Bit-Reversal Rearrangement algorithm is described in Algorithm \ref{Algo:Bit Reversal}.


\begin{algorithm}[h]
	\caption{\textsf{Bit Reverse Rearrangement} ($\fbu_\text{in}$)}
	\label{Algo:Bit Reversal}
	\begin{algorithmic}[1]
		\Require $\fbu_\text{in} = (c_0, c_1, \ldots, c_j, \ldots, c_{n-1})$, where $n = 2^m$, and each index $j$ is represented as an $m$-bit integer.
		\Ensure $\fbu_\text{out}$
		
		\For{$j = 0$ to $n - 1$}
		\State $j\text{-}\mathsf{rev} \gets 0$
		\State $j_{\text{tmp}} \gets j$
		\For{$k = 0$ to $m - 1$}
		\State $j\text{-}\mathsf{rev} \gets (j\text{-}\mathsf{rev} \ll 1) \ \text{OR} \ (j_{\text{tmp}} \ \text{AND} \ 1)$
		\State $j_{\text{tmp}} \gets j_{\text{tmp}} \gg 1$
		\EndFor
		\If{$j < j\text{-}\mathsf{rev}$}
		\State Swap $c_j$ and $c_{j\text{-}\mathsf{rev}}$
		\EndIf
		\EndFor
		
		\State \Return $\fbu_\text{out} \gets (c_0, c_1, \ldots, c_{n-1})$
	\end{algorithmic}
\end{algorithm}


% Next, we introduce the \textsf{Aggregate} module, the second building block of the Gao-Mateer algorithm.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}
	\caption{\textsf{Expand} ($\fbu_\text{in}, \theta, \{\beta_{0,0}, \ldots, \beta_{0, m-1}\}$)}
	\label{Algo:Expand}
	\begin{algorithmic}[1]
		\Require $\fbu_\text{in} = (c_0,c_1,\ldots,c_{n-1})$, which represents $f(x) \in \mathbb{F}[x]$ of degree $< n = 2^m$, 
		$\theta \in \mathbb{F}_{2^k}$ is the affine shift, 
		and $\beta_{0,i} \in \mathbb{F}_{2^k}$ are the basis of $W_m$.
		\Ensure $\fbu_\text{out}$, $\boldsymbol{\theta} = (\theta_0, \ldots, \theta_{m-1})$, 
		$\boldsymbol{\Gamma} = (\mathbf{G}_0 = \emptyset, \mathbf{G}_1, \ldots, \mathbf{G}_{m-1})$, 
		where $\theta_r$ and $\mathbf{G}_r = \{\gamma_{r,0},\ldots, \gamma_{r,r-1}\}$ denote the affine shift and basis corresponding to round $r$ of the \textsf{Aggregate} module.
		
		\For{$r = 0$ to $m-1$}
		\Comment{Scaling polynomials:}
		\State $\psi \gets 1$ \Comment{$\psi$ denotes the scaling factor of each term} \label{line:expand-psi-init}
		\State $\mathsf{offset} \gets 2^r$
		
		\While{$\mathsf{offset} \leq 2^m -1$}
		\For{$i = 0$ to $2^r - 1$}
		\State $c_{\mathsf{offset}+i} \gets c_{\mathsf{offset}+i} \times \psi$ \label{line:expand-scale}
		\EndFor
		\State $\psi \gets \psi \times \beta_{r,m-r-1}$ \label{line:expand-psi}
		\State $\mathsf{offset} \gets \mathsf{offset} + 2^r$
		\EndWhile
		
		\State $(c_0,\ldots,c_{n-1}) \gets$ \Call{Taylor Expansion}{$(c_0,\ldots,c_{n-1}), r$} \Comment{Algorithm \ref{Algo:TaylorExpansion}} \label{line:expand-taylor}
		
		\For{$i = 0$ to $m-r-2$}
		\State $\gamma_{m-r-1,i} \gets \beta_{r,i} \times \beta_{r,m-r-1}^{-1}$ \label{line:expand-gamma_computaion}   
		\State $\beta_{r+1,i} \gets \gamma_{m-r-1,i}^2 + \gamma_{m-r-1,i}$ \label{line:expand-beta_computaion}
		\EndFor
		\State $\mathbf{G_{m-r-1}} \gets (\gamma_{m-r-1,0}, \ldots, \gamma_{m-r-1,m-r-2})$
		\State $\theta_{m-r-1} \gets \theta \times \beta_{r,m-r-1}^{-1}$ \label{line:expand-theta1_computaion}
		\State $\theta \gets \theta_{m-r-1}^2 + \theta_{m-r-1}$ \label{line:expand-theta2_computaion}
		\EndFor
		
		\State $(c_0,\ldots,c_{n-1}) \gets$ \Call{Bit Reverse Rearrangement}{$(c_0,\ldots,c_{n-1})$}
		
		\State \Return $\fbu_\text{out} \gets (c_0,c_1,\ldots,c_{n-1})$
	\end{algorithmic}
\end{algorithm}






% Let \( f(x) \) be a polynomial of degree \( <n=2^m \), and let \( W_m = \langle \beta_0, \beta_1, \cdots, \beta_{m-1} \rangle \).



% In the original Gao FFT algorithm, as suggested in Algorithm \ref{Algo:Gao}, the $\textsf{TE}(\fbu, n)$ algorithm is applied to \( f(\beta_{m-1} x) \) and outputs $f_0(x)$ and $f_1(x)$ after the even-odd arrangements of $\fbu$ indices as described in the last step of Algorithm \ref{Algo:TaylorExpansion}. 

% Subsequently, again Algorithm \ref{Algo:TaylorExpansion} is separately applied to the first and second half of $\fbu$, representing two polynomials \( f_0(\beta_{1,m-2} x) \) and \( f_1(\beta_{1,m-2} x) \), each of degree \( <n=2^{m-1} \), where \( \beta_{1,m-2} = (\gamma_{m-1})^2 + \gamma_{m-1} \) and \( \gamma_{m-1} = \beta_{m-2}\beta_{m-1}^{-1} \). 

% Alternatively, instead of 

% Subsequently, the \textsf{TE} algorithm is applied to polynomials of smaller degree, consistent with the original Gao FFT algorithm.

% The subsequent rounds of the \textsf{Expand}  module, which is explained in the next section, of half size is applied to the outputted polynomials.






% employs the Taylor expansion algorithm to divide $2^r$ polynomials of degree $< 2^{m-r}$ into $2^{r+1}$ smaller polynomials of degree $<2^{m-r-1}$. 

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.9\linewidth} % Correct usage of width specifier
		\centering
		\includegraphics[width=0.9\linewidth]{Figures/Gao_expand.pdf} 
		\caption{ Original Approach: Groups even and odd indices after each Taylor expansion}
		\label{fig:gao-expand-old}
	\end{subfigure}
	\vspace{1em} % Adjust the space between figures if necessary
	\begin{subfigure}[b]{0.9\linewidth} % Correct usage of width specifier
		\centering
		\includegraphics[width=0.9\linewidth]{Figures/Gao_expand_binary.pdf} 
		\caption{ Alternative Approach: Rearranges the vector in bit-reversal order in the final round}
		\label{fig:gao-expand-binary}
	\end{subfigure}
	
	\caption[The \textsf{Expand} Module in the Gao FFT Algorithm]{The \textsf{Expand} module in the Gao FFT algorithm of length $n=2^3$, which evaluates a polynomial $f(x) \in \mathbb{F}[x]$ of degree $< 8$ over $\theta + W_3$, where $W_3 = \langle \beta_0, \beta_1, \beta_2 \rangle$. 
		In Figure (a), the \textsf{Expand} module groups even and odd indices after each Taylor expansion. In contrast, Figure (b) skips these rearrangements by using one $\textsf{T}_8$ module instead of two $\textsf{T}_4$ modules in round $r=1$ then performs the bit-reversal in the final round. 
		 Additionally, in this figure, $\beta_{0,2} = \beta_2$, and $\beta_{1,1} = (\beta_1 \beta_2^{-1})^2 -  (\beta_1 \beta_2^{-1})$. Given that $\beta_{1,0} = (\beta_0 \beta_1^{-1})^2 -  (\beta_0 \beta_1^{-1})$, we can compute $\beta_{2,0} = (\beta_{1,0} \beta_{1,1}^{-1})^2 -  (\beta_{1,0} \beta_{1,1}^{-1})$.
	}
	
	\label{fig:gao-expand}
\end{figure}

\clearpage
\subsection{Aggregate Module}

The \textsf{Aggregate} module combines $2^r$ adjacent elements in $\fbu$ during round $r$, where $0 \leq r \leq m-1$. Let $\{\eta_0, \eta_1, \ldots, \eta_{2^{r}-1} \}$ denote the elements in the affine subspace $\theta_r + \langle \gamma_{r,0}, \ldots, \gamma_{r,r-1} \rangle$ provided in the \textsf{Expand} module. At each round $r$, it computes those elements by spanning the mentioned affine subspace, which requires a total of $\sum_{i=0}^{r-1} 2^i = 2^r -1$ finite field additions and $2^r + r$ memory accesses: $r$ memory reads for retrieving the $\gamma_i$s and $2^r$ memory writes for storing the $\eta_k$s. The algorithm for computing $\eta_k$s is described in Algorithm \ref{Algo:Span}.

% Algorithm \ref{Algo:Aggregate} describes the textsf{Aggregate} module te\tex. 

\begin{algorithm}[htb]
	\caption{\textsf{Span} ($\mathbf{G} = \{\gamma_0, \ldots, \gamma_{r-1}\}, \theta$)}
	\label{Algo:Span}
	\begin{algorithmic}[1]
		\Require $\mathbf{G} = \{\gamma_0, \ldots, \gamma_{r-1}\}$ is a linearly independent set over $\mathbb{F}_2$, and $\theta \in \mathbb{F}_{2^k}$ is an affine shift.
		\Ensure $\{\eta_0, \eta_1, \ldots, \eta_{2^r - 1}\} = \theta + \langle \gamma_0, \ldots, \gamma_{r-1} \rangle$
		
		\State $\eta_0 \gets \theta$
		
		\If{$\mathbf{G} = \emptyset$}
		\State \Return $\{\eta_0\}$
		\EndIf
		
		\For{$i = 0$ to $r - 1$}
		\For{$k = 0$ to $2^i - 1$}
		\State $\eta_{k + 2^i} \gets \eta_k + \gamma_i$
		\EndFor
		\EndFor
		
		\State \Return $\{\eta_0, \eta_1, \ldots, \eta_{2^r - 1}\}$
	\end{algorithmic}
\end{algorithm}


\begin{algorithm}[htb]
	\caption{\textsf{Aggregate} ($\fbu_\text{in}, \boldsymbol{\Gamma}, \boldsymbol{\theta}$)}
	\label{Algo:Aggregate}
	\begin{algorithmic}[1]
		\Require $\fbu_\text{in} = (c_0,c_1,\ldots,c_{n-1})$ is a vector of length $ n = 2^m$, 
		$\boldsymbol{\theta} = (\theta_0, \ldots, \theta_{m-1})$, 
		$\boldsymbol{\Gamma} = (\mathbf{G}_0 = \emptyset, \mathbf{G}_1, \ldots, \mathbf{G}_{m-1})$, 
		where $\theta_r$ and $\mathbf{G}_r = \{\gamma_{r,0},\ldots, \gamma_{r,r-1}\}$ denote the affine shift and basis corresponding to round $r$.
		\Ensure $\fbu_\text{out}$ is the vector of evaluations of $f(x)$ over $\theta + W_m$.
		
		\For{$r = 0$ to $m-1$}
		\State $\{\eta_0, \eta_1, \ldots, \eta_{2^{r}-1} \} \gets \mathsf{Span}(\mathbf{G}_r, \theta_r)$
		\For{$j = 0$ to $2^{m-r-1}-1$}
		\State $d \gets j \cdot 2^{r+1}$
		\For{$i = 0$ to $2^r-1$}
		\State $c_{d+i} \gets c_{d+i} + c_{d+2^r+i} \times \eta_i$ \label{Step:AlgAgreeM1}
		\State $c_{d+2^r+i} \gets c_{d+2^r+i} + c_{d+i}$ \label{Step:AlgAgreeM2}
		\EndFor
		\EndFor
		\EndFor
		
		\State \Return $\fbu_\text{out} \gets (c_0,c_1,\ldots,c_{n-1})$
	\end{algorithmic}
\end{algorithm}



\subsection{Detailed Cost Analysis}\label{Sec:Cost-GM}
We now compute the number of multiplications and additions required by the algorithm. From Algorithm~\ref{Algo:Expand}, we know that at the $r$th iteration, we need to scale $2^r$ polynomials, each of degree $2^{m-r}$. Thus, the multiplication for the scaling is given by
$\sum_{r=0}^{m-1} 2^r \cdot (2^{m-r}-1)= 2^m m - \sum_{r=0}^{m-1} 2^r= n \log_2 n -n +1$.

From Algorithm~\ref{Algo:Aggregate}, we know that at the $r$th iteration, the number of required multiplications is $2^r \cdot 2^{m-r-1}=2^{m-1}$. Thus, the total multiplication cost in Algorithm~\ref{Algo:Aggregate} is $\sum_{r=0}^{m-1} 2^{m-1}=\frac{1}{2} n \log_2 n.$ Therefore, the total number of multiplications in the Gao-Mateer algorithm is given by $n \log_2 n -n +1 + \frac{1}{2} n \log_2 n = \frac{3}{2} n \log_2 n -n +1$.
% \[n \log_2 n -n +1 + \frac{1}{2} n \log_2 n = \frac{3}{2} n \log_2 n -n +1 .\]

From Algorithm~\ref{Algo:TaylorExpansion}, we know that in the $r$th iteration, the number of additions due to the Taylor expansion is $2^{m-1}(m-r-1)$. Thus, the total number of additions required for the Taylor expansions is $\sum_{r=0}^{m-2}2^{m-1}(m-r-1) = 2^{m-2}m(m-1)$. Also, in Algorithm~\ref{Algo:Aggregate}, we know that at the $r$th iteration, the number of required additions is $2 \cdot 2^r \cdot 2^{m-r-1} =2^{m}$. Therefore, the total addition cost for the algorithm is given by
\(2^{m-2}m(m-1) + m\cdot 2^m= \frac{1}{4}n(\log_2n)^2 + \frac{3}{4}n\log_2n. \)

When performing the FFT over a subspace, at each $r$th iteration of Algorithm~\ref{Algo:Aggregate}, we have $\eta_0=0$. Thus, for Steps~\ref{Step:AlgAgreeM1} and \ref{Step:AlgAgreeM2}, we need no multiplications are required, and only one addition is needed. Consequently, we must account for a reduction of $\sum_{r=0}^{m-1} 2^{m-r-1}=2^m-1=n-1$ in both additions and multiplications. Therefore, the costs for multiplications and additions are changed to
$\frac{3}{2} n \log_2 n -2n +2$ and  $\frac{1}{4}n(\log_2n)^2 + \frac{3}{4}n\log_2n-n+1$, respectively.

\subsection{Optimization for Cantor Special Basis}\label{Sec:Gao-optimization-Cantor_basis}
% We know that evaluating $f(x)$ over $\theta + W_m$ is equivalent to evaluating $g(x)$ over $(\theta_0 + G) \cup (1 + \theta_0 + G)$, where $\theta_0 = \beta_{m-1}^{-1} \theta$. Thus, to compute $g(x)=f(\beta_{m-1}x)$, we need to scale the function $f(x)$, which requires $n-1$ multiplications. However, by simply choosing $\beta_{m-1} = 1$, we can eliminate these $n-1$ multiplications. However, for the next iteration, we need to scale the functions $f_0(x)$ and $f_1(x)$ by $\beta_{m-1}^2 + \beta_{m-1}$, as we need to evaluate $f_0(x)$ and $f_1(x)$ over $\theta_0^2 + \theta_0 + D$. 
% where
% \[
%     D = \langle \alpha^{2(m-1)} + \alpha^{m-1}, \ldots, \alpha^2 + \alpha \rangle.
% \]
% Thus, we need $2 \times (2^{m-1} - 1) = 2^m - 2$ multiplications for the scaling. This scaling is required in each iteration until $D$ is reduced to dimension 1. The interesting question is whether we can avoid scaling in every iteration.

If we have a Cantor special basis of dimension $m$, we can avoid the scaling in every iteration. We know that a Cantor special basis satisfy the following

\begin{equation*}
	\begin{aligned}
		&\beta_{0}=1 \quad \text{and } S(\beta_{i})=\beta_{i}^2+\beta_{i}=\beta_{i-1} \quad \text{for } 1\leq i \leq m-1,
	\end{aligned}
\end{equation*}
where $S(x)=x^2+x$. In addition, we know that $S^{i}(\beta_{i})=\beta_{0}=1$ for $0 \leq i \leq m-1$ and $S^{i+\ell}(\beta_{i+\ell})=\beta_{\ell}$ for any $i,\ell \geq 0$ with $i+\ell\leq m-1$. Now, consider the Cantor special basis in the reversed order, i.e.,
\begin{equation*}
	\begin{aligned}
		W_m
		&= \langle \beta_{m-1},\beta_{m-2},\ldots,\beta_{1},1  \rangle= \langle \beta_{m-1},S(\beta_{m-1}),\ldots,S^{m-2}(\beta_{m-1}),1  \rangle.
	\end{aligned}
\end{equation*}

Thus, we have $G= \langle \beta_{m-1},S(\beta_{m-1}),\ldots,S^{m-2}(\beta_{m-1}))$ and 
\begin{equation*}
	\begin{aligned}
		D
		&= \langle S(\beta_{m-1}),S^2(\beta_{m-1}),\ldots,S^{m-1}(\beta_{m-1}))
		= \langle S(\beta_{m-1}),S^2(\beta_{m-1}),\ldots,1 \rangle.
	\end{aligned}
\end{equation*}

Thus, we do not need the scaling for the functions $f_0(x)$ and $f_1(x)$. Also, at the $j$-th iteration, $G$ and $D$ will be of the form 
\begin{equation*}
	\begin{aligned}
		G^{(j)}
		&= \langle S^{j}(\beta_{m-1}),S^{j+1}(\beta_{m-1}),\ldots,S^{m-2}(\beta_{m-1}) \rangle \quad \text{ and } \\
		D^{(j)}
		&= \langle S^{j+1}(\beta_{m-1}),S^{j+2}(\beta_{m-1}),\ldots,S^{m-1}(\beta_{m-1})=1 \rangle.
	\end{aligned}
\end{equation*}

Therefore, at each iteration there is no need for scaling. Also, due to the chosen basis, computing the basis elements in $G^{(j)}$ and $D^{(j)}$ does not require any multiplications or additions. This can be done simply by selecting one fewer element from $G^{(j-1)}$ and $D^{(j-1)}$.

\paragraph{Detailed cost analysis of the optimized algorithm} When using the Cantor basis in Algorithm~\ref{Algo:Expand}, no scaling is required for the polynomials. All other steps in Algorithms~\ref{Algo:Expand} and \ref{Algo:Aggregate} remain unchanged. Thus, the number of additions remains the same, as evaluated in Section~\ref{Sec:Cost-GM}. Consequently, the multiplication and addition costs are $\frac{1}{2}n\log_2n$ and $\frac{1}{4}n(\log_2 n)^2 + \frac{3}{4}n\log_2 n$, respectively.

% The number of additions remains unchanged, as evaluated in Section~\ref{Sec:Cost-GM}. Therefore, the total addition cost for the optimized algorithm is $\frac{1}{4}n(\log_2 n)^2 + \frac{3}{4}n\log_2 n$.

Furthermore, as discussed in Section~\ref{Sec:Cost-GM}, when performing the FFT over a subspace, the total multiplication and addition costs for the algorithm are $\frac{1}{2}n\log_2n - n+1$ and $\frac{1}{4}n(\log_2n)^2 + \frac{3}{4}n\log_2n - n +1$, respectively.

Thus, by using Cantor special basis in the Gao-Mateer algorithm, we can efficiently compute the additive FFT of $f(x)\in \F_{2^k}[x]$ over $\theta + W_m$ where $\F_{2^k}$ contain a subfield $\F_{2^{2^\ell}}$ with $m \leq 2^{\ell}$.

% Table~\ref{tab:FFTcomparisons} presents a comparison of the number of additions and multiplications required by the Cantor, Gao-Mateer, and LCH~\cite{LCH-FFT2016} FFTs with precomputation.

% \begin{remark}
	%     Note that the second algorithm in~\cite{Gao2010FFT} applies to the additive FFT of length $n = 2^m$, where $m = 2^t$, and utilizes the Cantor special basis. However, for a polynomial $f(x)$ of degree between $2^{2^{t-1}}$ and $2^{2^t}$, particularly when their degree is significantly less than $2^{2^t}$, a large number of zero-padding is required to extend the polynomial vector $\mathbf{f}$ to length $2^{2^t}$, resulting in unnecessary computational overhead. In contrast, our approach, which uses the Cantor special basis in their first algorithm, allows for handling polynomials of any degree for arbitrary values of $m$ without this restriction.
	% \end{remark}

% When we do the evaluation over a subspace, in the last iteration, the evaluation of $f(x)$ requires no multiplication and one additions which implies that the last iteration involves no multiplication and $2^{m-1}$ additions. Therefore, the total cost of multiplication and addition for the algorithm is $\frac{1}{2}n\log_2n-\frac{1}{2}n$ and $\frac{1}{4}n(\log_2n)^2 + \frac{3}{4}n\log_2n-\frac{1}{2}$, respectively.

\subsection{Precomputation}\label{sec:gao-precmp}

In this section, we introduce two levels of precomputation. The first level precomputes the set $\boldsymbol{\beta} = \{\beta_{0, m-1}, \ldots, \beta_{r, m-r-1}, \ldots, \beta_{m-1, 0} \}$, which is essential for determining the scaling factors. Additionally, it precomputes  $\boldsymbol{\Gamma} = (\mathbf{G}_0 = \emptyset, \mathbf{G}_1,  \ldots, \mathbf{G}_{m-1})$ and $\boldsymbol{\theta} = (\theta_0, \ldots, \theta_{m-1})$ required by the \textsf{Aggregate} module, initially provided by the \textsf{Expand} module. This precomputation requires $m$ finite field elements for $\boldsymbol{\beta}$ and $\boldsymbol{\theta}$, along with $\sum_{i=0}^{m-1} i = m(m-1)/2$ finite field elements for $\boldsymbol{\Gamma}$. Consequently, this algorithm stores a total of $(m^2+3m)/2$  finite field elements. 
For the  variant of the Gao-Mateer algorithm described in Section~\ref{Sec:Gao-optimization-Cantor_basis}, the scaling operation is omitted, and each basis $\mathbf{G}_r$ is defined as $\{\beta_0, \beta_1, \ldots, \beta_{r-1}\}$. Therefore, this level of precomputation is not applicable for that.

\begin{algorithm}[htb]
	\caption{\small Gao-Mateer Precomputation Level 1 ($\theta$, $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$)}
	\label{Algo:Gao_Precmp_lvl1}
	\begin{algorithmic}[1]
		\Require $\theta \in \mathbb{F}_{2^k}$ is the affine shift, and $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$ is a linearly independent set over $\mathbb{F}_2$
		\Ensure $\boldsymbol{\beta} = \{\beta_{0, m-1}, \ldots, \beta_{r, m-r-1}, \ldots, \beta_{m-1, 0} \}$,  
		$\boldsymbol{\theta} = (\theta_0, \ldots, \theta_{m-1})$,  
		$\boldsymbol{\Gamma} = (\mathbf{G}_0 = \emptyset, \mathbf{G}_1, \ldots, \mathbf{G}_{m-1})$
		
		\State $\beta_{0, m-1} \gets \beta_{m-1}$
		\State $\mathbf{G}_0 \gets \emptyset$
		
		\For{$r = 0$ to $m - 2$}
		\For{$i = 0$ to $m - r - 2$}
		\State $\gamma_{m - r - 1, i} \gets \beta_{r, i} \times \beta_{r, m - r - 1}^{-1}$
		\State $\beta_{r + 1, i} \gets \gamma_{m - r - 1, i}^2 + \gamma_{m - r - 1, i}$
		\EndFor
		\State $\mathbf{G}_{m - r - 1} \gets (\gamma_{m - r - 1, 0}, \ldots, \gamma_{m - r - 1, m - r - 2})$
		\State $\theta_{m - r - 1} \gets \theta \times \beta_{r, m - r - 1}^{-1}$
		\State $\theta \gets \theta_{m - r - 1}^2 + \theta_{m - r - 1}$
		\EndFor
		
		\State \Return $\boldsymbol{\beta} = \{\beta_{0, m-1}, \ldots, \beta_{r, m-r-1}, \ldots, \beta_{m-1, 0} \}$,  
		$\boldsymbol{\theta} = (\theta_0, \ldots, \theta_{m-1})$,  
		$\boldsymbol{\Gamma} = (\mathbf{G}_0 = \emptyset, \mathbf{G}_1, \ldots, \mathbf{G}_{m-1})$
	\end{algorithmic}
\end{algorithm}


The second level of precomputation precomputes the powers of the actual scaling factors. Moreover, instead of storing the basis $\mathbf{G}_r$, it precomputes all elements in each affine subspace $\theta_r + \langle \gamma_{r,0}, \ldots, \gamma_{r,r-1} \rangle$. This requires storing $\sum_{r=0}^{m-1} (2^{m-r} - 1) = 2^{m+1} - m - 2$ finite field elements for all scaling factors stored in $\boldsymbol{\Psi} = (\boldsymbol{\psi}_0, \ldots, \boldsymbol{\psi}_{m-1})$ where $\boldsymbol{\psi}_r = (\psi_{r,1}, \ldots,   \psi_{r,2^{m-r}-1})$ and $\psi_{r,d} = \beta_{r, m-r-1}^d$. 
Also, it stores $\sum_{r=0}^{m-1} 2^r = 2^m - 1$ finite field elements for all computed elements stored in $\mathbf{H} = (\boldsymbol{\eta}_0, \ldots, \boldsymbol{\eta}_{m-1})$, where $\boldsymbol{\eta}_r = \{\eta_0, \ldots, \eta_{2^r-1}\}$ are elements in $\theta_r + \langle \gamma_{r,0}, \ldots, \gamma_{r,r-1} \rangle$. Consequently, this algorithm stores a total of $3 \cdot 2^m - m - 3$ finite field elements. For the optimized variant described in Section~\ref{Sec:Gao-optimization-Cantor_basis}, the scaling factor is not required, and we only need to obtain the elements in $\theta + \langle\beta_0, \beta_1, \ldots, \beta_{m-1}\rangle$, which equals $2^m$ finite field elements.

\begin{algorithm}[htb]
	\caption{\small Gao-Mateer Precomputation Level 2 ($\theta$, $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$)}
	\label{Algo:Gao_Precmp_lvl2}
	\begin{algorithmic}[1]
		\Require $\theta \in \mathbb{F}_{2^k}$ is the affine shift, and $\{\beta_0, \beta_1, \ldots, \beta_{m-1}\}$ is a linearly independent set over $\mathbb{F}_2$
		\Ensure $\boldsymbol{\Psi} = (\boldsymbol{\psi}_0, \ldots, \boldsymbol{\psi}_{m-1})$,  
		$\mathbf{H} = (\boldsymbol{\eta}_0, \ldots, \boldsymbol{\eta}_{m-1})$,  
		where each $\boldsymbol{\eta}_r = \{\eta_0, \ldots, \eta_{2^r-1}\}$ is the output of \Call{Span}{$\mathbf{G}_r, \theta_r$}
		
		\State $\boldsymbol{\beta}, \boldsymbol{\theta}, \boldsymbol{\Gamma} \gets$ \Call{Gao Precomputation Level 1}{$\theta$, $\{\beta_0, \ldots, \beta_{m-1}\}$}
		
		\For{$r = 0$ to $m - 1$}
		\State $\psi_{r,0} \gets 1$ \Comment{$\psi$ is the scaling factor of round $r$}
		\For{$j = 1$ to $2^{m - r} - 1$}
		\State $\psi_{r,j} \gets \psi_{r,j - 1} \times \beta_{r, m - r - 1}$
		\EndFor
		\State $\boldsymbol{\psi}_r \gets (\psi_{r,1}, \ldots, \psi_{r,2^{m - r} - 1})$ \Comment{Note: $\psi_{r,0} = 1$ is skipped}
		\State $\boldsymbol{\eta}_r \gets$ \Call{Span}{$\mathbf{G}_r, \theta_r$}
		\EndFor
		
		\State $\boldsymbol{\Psi} \gets (\boldsymbol{\psi}_0, \ldots, \boldsymbol{\psi}_{m - 1})$
		\State $\mathbf{H} \gets (\boldsymbol{\eta}_0, \ldots, \boldsymbol{\eta}_{m - 1})$
		
		\State \Return $\boldsymbol{\Psi}$, $\mathbf{H}$
	\end{algorithmic}
\end{algorithm}


Algorithms~\ref{Algo:Gao_Precmp_lvl1} and \ref{Algo:Gao_Precmp_lvl2} summarize those precomputations.


\section{Aurora FFT Complexity Analysis} \label{Sec:FFT_Calls_in_Aurora}
Given a target security parameter $\boldsymbol{\lambda}$, and subsets $H_1$ and $H_2$,
% where size of each is determined by the number of constraints and variables of a given R1CS relation respectively,
the size of the codeword domain (i.e., $|L|$) is determined. This size is important for the analysis of the FFT complexity of Aurora, as the input length of many FFT and IFFT instances is $|L|$. Before computing $|L|$, we need to review how the number of queries to the  codeword is determined based on the target security parameter.

\subsection{Soundness Errors: Queries and Repetition Analysis}
Given $\boldsymbol{\lambda}$, $\boldsymbol{\epsilon_q}$ and $\boldsymbol{\epsilon_i}$ represent query and interactive soundness errors such that
\(
\boldsymbol{\epsilon_q} + \boldsymbol{\epsilon_i} < 2^{-\boldsymbol{\lambda}},
\)
where $2^{-\boldsymbol{\lambda} - 1}$ is allocated to each. According to \cite[Theorem 4]{Aurora2019}, 
\[
\boldsymbol{\epsilon_i} = \left(\frac{\eta + 1}{|\mathbb{F}|}\right)^{\lambda_i} + \left(\frac{|L|}{|\mathbb{F}|}\right)^{\lambda'_i} + \epsilon^{\text{FRI}}_i,\text{ and } \boldsymbol{\epsilon_q} = \epsilon^{\text{FRI}}_q,
\]
where $ \left(\frac{\eta + 1}{|\mathbb{F}|}\right)^{\lambda_i}$ and $\left(\frac{|L|}{|\mathbb{F}|}\right)^{\lambda'_i}$ denote the lincheck and LDT soundness errors respectively. $\epsilon^{\text{FRI}}_i$ and   $\epsilon^{\text{FRI}}_q$ denote the interactive and query soundness errors in FRI, respectively. Each term in $\boldsymbol{\epsilon_i}$ gets $2^{-\boldsymbol{\lambda} - 3}$ and $\boldsymbol{\epsilon_q}$ gets $2^{-\boldsymbol{\lambda} - 1}$ soundness error bound. 
The codeword is queried during FRI. 
% where the number of queries is determined by $\epsilon^{\text{FRI}}_q$ and the localization parameter $\phi$. 
Given, the target proximity parameter in (\ref{eq:proximity_parameter}), the number of query repetitions in FRI is:
\begin{equation}\label{eq:num_queries}
	\lambda_q^\text{FRI} = \frac{\log(\epsilon^{\text{FRI}}_q)}{\log\left( 1 - \min\left(\delta,  \frac{1- 3\rho - 2^\phi/\sqrt{|L|}}{4} \right)  \right)}.  
\end{equation}
Moreover, the required lincheck and LDT repetition parameters are determined as:
\(
\lambda_i = \frac{-\boldsymbol{\lambda} - 3}{\log\left(\frac{\eta+1}{|\mathbb{F}|}\right)},\text{ and } \lambda'_i = \frac{-\boldsymbol{\lambda} - 3}{\log\left(\frac{|L|}{|\mathbb{F}|}\right)}.
\)



\subsection{Codeword Size}
The maximum degree of the polynomial in the codeword of size $|L|$ is $d = 2t+2\texttt{b}$ (according to Table \ref{tab:codewords}), where \texttt{b} is determined by the expected number of queries to the corresponding codeword.
% , to let \texttt{b} evaluations of the polynomial over $|L|$ be randomly distributed to achieve zero-knowledge. 
The number of queries to the codeword is $\texttt{b} = \lambda_q^\text{FRI} \cdot 2^{\phi}$, where according to (\ref{eq:num_queries}), $\lambda_q^\text{FRI}$ also relies on $|L|$. To solve this loop, we initialize $|L| = 4t/\rho$ and \texttt{b} as mentioned. Then, we check the following condition: Let $\texttt{r}=\lfloor\log(\rho|L|)/\phi\rfloor$ denote the number of reductions in FRI,  $d' = 2t + \texttt{b}-1$ (lincheck maximum degree) must be divisible by $2^{\texttt{r}\phi}$, otherwise, $d'$ must be increased to the nearest multiple of $2^{\texttt{r}\phi}$. Then, we check if the new $d' \leq \rho |L|$, otherwise, increase the dimension of $L$ by one. Again, we compute \texttt{b} and check if it is the same as it was; otherwise, we update that and again check the condition with the new $L$. This iteration is continued until the \texttt{b} remains unchanged, at which point $|L|$ is determined.

\subsection{Construction of $H_1$, $H_2$, and $L$}
Let $\{\beta_0, \beta_1, \dots, \beta_{k-1}\}$ represent basis elements of $\mathbb{F}_{2^k}$. Then, we construct subsets  $H_1=\langle \beta_0, \beta_1, \cdots \beta_{\lceil\log\eta\rceil} \rangle$ and $H_2 = \langle \beta_0, \beta_1, \cdots \beta_{
	\lceil\log(\mu + 1)\rceil} \rangle$ and the affine subspace $L= \beta_{
	\lceil\log(|L|)\rceil+1} + \langle \beta_0, \beta_1, \cdots \beta_{
	\lceil\log(|L|)\rceil} \rangle$. In this way,  $L \cap (H_1 \cup H_2) = \emptyset$. If the basis elements are Cantor special basis, the Cantor FFT can be used without precomputation due to this special construction, as detailed in Section \ref{sec:cantor_precmp}.

\subsection{FFT/IFFT Calls}
Table \ref{tab:fftcalls} summarizes the FFT and IFFT calls for computing the codewords in Table \ref{tab:codewords}. Virtual oracles are used for consistency checks, and are not included in the random combination of codewords input to the FRI protocol.

\begin{table}
	\centering
	\caption{ The FFT and IFFT calls in the Aurora zkSNARK protocol}
	{
		\label{tab:fftcalls}
		\begin{tabularx}{\linewidth}{>{\hsize=.38\hsize}XX}
			\toprule
			\textbf{FFT Calls} & \textbf{Description} \\
			\midrule
			$\lambda_i \times$ FFT of len. $|L|$ & Compute the masking oracle $\hat{\rbu}_\ell$: Evaluate the polynomial $r_\ell$ of degree $<2t+b-1$ over $L$, where $\ell \in [1,\lambda_i]$. \\
			\midrule
			
			(1) IFFT of len. $\kappa + 1$\newline
			(2) FFT of len. $\mu + 1$\newline
			(3) IFFT of len. $\mu + 1$\newline
			(4) FFT of len $|L|$
			& Compute the oracle $\hat{\fbu}_\wbu$: \newline
			(1) Interpolate $\vbu$ to get $f_{(1,\vbu)}$ of degree $<\kappa + 1$. \newline
			(2) Evaluate $f_{(1,\vbu)}$ over $H_2$. \newline 
			Then, computing $\fbu'_\wbu = {\wbu_{i-\kappa-1}[0:\mu-\kappa - 1] - \fbu_{(1,\vbu)}[\kappa+1:\mu]}$ \newline
			% $f_\wbu$ of degree $<\mu - \kappa$ \newline 
			(3) Interpolate $\fbu'_\wbu$ over $H_2$ to get $f'_\wbu$ of degree $< \mu + 1$. \newline
			Then, divide $f'_\wbu$ by $\mathbb{Z}_{\{h_0,\dots, h_{\kappa}\}}$ to get $f_\wbu$.\newline
			(4) Evaluate $f_\wbu^*$ over $L$.
			\\
			\midrule
			(1) $3 \times$ IFFT of len. $\eta$\newline
			(2) $3 \times$ FFT of len. $|L|$
			&
			Compute the oracles $\hat{\fbu}_{\mathbf{Az}}$, $\hat{\fbu}_{\mathbf{Bz}}$, and $\hat{\fbu}_{\mathbf{Cz}}$:\newline
			(1) Interpolate $\mathbf{Az}$, $\mathbf{Bz}$, and $\mathbf{Cz}$ to get $f_\mathbf{Az}$, $f_\mathbf{Bz}$, $f_\mathbf{Cz}$ of degree $<\eta$.\newline
			(2) Evaluate $f_\mathbf{Az}^*$, $f_\mathbf{Bz}^*$, $f_\mathbf{Cz}^*$ over $L$.
			\\
			\midrule
			$\lambda'_i \times$ FFT of len. $|L|$ & Compute the masking oracle $\hat{\rbu}'_\ell$: Evaluate the polynomial $r'_\ell$ of degree $<2t+2b$ over $L$, where $\ell \in [1,\lambda'_i]$. \\
			\midrule
			$\lambda_i \times$ IFFT of len. $t$ & Interpolate the polynomial $p_{\alpha_\ell}$ in (\ref{eq:pa}), where $\ell \in [1,\lambda_i]$. 
			\\
			\midrule
			$\lambda_i \times$ IFFT of len. $t$ & Interpolate the polynomial $p^{ABC}_{\alpha_\ell}$ in (\ref{eq:pa}), where $\ell \in [1,\lambda_i]$. 
			\\
			\midrule
			FFT of len. $|L|$ & Compute the virtual oracle $\hat{\fbu}_\zbu := \hat{\fbu}_\wbu \cdot \mathbb{Z}_{\{h_0,\dots, h_{\kappa}\}} + \hat{\fbu}_{(1,\vbu)}(h_i)$ which requires 
			Evaluating $f_{(1,\vbu)}$ over $L$.
			\\
			\midrule
			$\lambda_i \times 2 \times $FFT of len. $|L|$ & Compute virtual oracles $\hat{\mathbf{q}}_\ell^M := \hat{\fbu}_{\mathbf{Mz}}\cdot \hat{\mathbf{p}}_{\alpha_\ell} - \hat{\fbu}_\zbu \cdot \hat{\mathbf{p}}^{ABC}_{\alpha_\ell} $, where $\ell \in [1,\lambda_i]$ and $M \in \{A, B, C\}$, by evaluating $p_{\alpha_\ell}$ and $p^{ABC}_{\alpha_\ell}$ over $L$.
			\\
			\midrule
			(1) $\lambda_i \times $IFFT of len. $d$\newline where $d = 2^{\lceil\log(t+b)\rceil}$ \newline
			(2) $\lambda_i \times $FFT of len.
			$|L|$
			& Compute the oracle $\hat{\hbu}_\ell$:\newline
			(1) Interpolate $\sum_{M \in \{A, B, C\}} s^M_\ell \hat{\mathbf{q}}_\ell^M$ to get a  polynomial of degree less than the minimum power of two greater than $t + \texttt{b}$.%\newline
			Then, compute the polynomial $h$ according to Table \ref{tab:codewords}\newline
			(2) Evaluate $h$ over $L$
			\\
			\bottomrule
		\end{tabularx}
	}
\end{table}




\section{Complexity Analysis and Benchmarking}\label{Sec:ComplexityAnalysis}
In this section, we provide a comprehensive comparison of the Cantor and the Gao-Mateer algorithms, along with their respective variants. The short names for these algorithms are summarized in Table \ref{tab:alg_shortnames}, and will be referenced throughout this section for clarity.  


\begin{table}[h]
	\centering
	\caption{Short names for the algorithms used in the comparison.}
	% \renewcommand{\arraystretch}{1.3}
	\setlength{\tabcolsep}{8pt} % Adjusted column spacing
%	\resizebox{0.9\textwidth}{!}{
		\begin{tabular}{ll}
			\toprule
			\textbf{Name} & \textbf{Description} \\
			\midrule
			Cantor & Cantor Algorithm  [Section \ref{Sec:Cantor Implementation}]\\
			Cantor PC & Cantor Algorithm with precomputation [Section \ref{sec:cantor_precmp}] \\
			GM & Gao-Mateer algorithm (implemented  libiop \cite{libiop}) \\
			GM PCL1 & Gao-Mateer algorithm with level 1 precomputation [Section \ref{sec:gao-precmp}] \\
			GM PCL2& Gao-Mateer algorithm with level 2 precomputation [Section \ref{sec:gao-precmp}] \\
			GM CO & Gao-Mateer algorithm using Cantor Basis (Cantor Optimized) \\
			GM CO PCL2& Gao-Mateer algorithm using Cantor Basis with level 2 precomputation \\
			\bottomrule
		\end{tabular}
%	}
	\label{tab:alg_shortnames}
\end{table}

Table \ref{tab:FFTcomparisons} presents the number of additions and multiplications for the precomputed variant of the FFT algorithms, while Table \ref{tab:precmp_comparison} compares the space requirements of each precomputation method. Figure~\ref{fig:sub_parts_normalized} illustrates the contribution of each sub-algorithm in the Gao-Mateer FFT implementation from \cite{libiop}. The basis computations, which are precomputed in GM PCL1, require fewer resources as \( m \) increases. In contrast, the computational cost of bit-reversal rearrangement and Taylor expansion grows more rapidly for larger \( m \), significantly impacting the overall FFT runtime.


\begin{table}[h]
	\centering
	\caption{Comparison of the number of additions and multiplications}
	\renewcommand{\arraystretch}{1.5}  % Adjust the value to control the row height (default is 1.0)
	{
		\begin{tabular}{lcc}
			\toprule
			\textbf{Name} &  \textbf{\# Additions} & \textbf{\# Multiplications} \\
			\midrule
			% Cantor  & & \\
			% \hline
			Cantor PC & $\frac{1}{2} n\log_2 n + \frac{1}{2}n\sum_{r=0}^{\log_2 (n)-1} 2^{\text{wt}(r)}$  & $\frac{1}{2} n\log_2 n$ \\
%			\hline
			% GM & & \\
			% \hline
			% GM PCL1 & & \\
			% \hline
			GM PCL2  & $\frac{1}{4}n(\log_2n)^2 + \frac{3}{4}n\log_2$  & $\frac{3}{2} n \log_2 n -n +1$ \\
%			\hline  
			% GM CO & & \\
			% \hline
			GM CO PCL2 & $\frac{1}{4}n(\log_2n)^2 + \frac{3}{4}n\log_2$ & $\frac{1}{2} n\log_2 n$\\
%			\hline
			% LCH & $ ?$  &  $?$\\
			% \hline
			% LCH PC & $O(n\log_2 n\log\log_2 n)+  n\log_2 n$ \textsuperscript{*} &  $\frac{1}{2} n\log_2 n$\\
			 \bottomrule
		\end{tabular}
	}
	\label{tab:FFTcomparisons}
\end{table}


\begin{table}
	\centering
	\caption{The storage required for precomputation of each algorithm}
	\setlength{\tabcolsep}{8pt} % Adjusted column spacing
%	\resizebox{0.9\textwidth}{!}{
		\begin{tabular}{ll}
			\toprule
			\textbf{Method} & \textbf{Storage} \\
			\midrule
			Cantor PC & $2^m - 1$ elements in $\mathbb{F}_{2^k}$ and $\sum_{i=0}^{m-1} \left(2^{\text{wt}(i)} - 1\right)$ integers [Section \ref{sec:cantor_precmp}]\\ 
			GM PCL1 & $(m^2 + 3m) / 2$ elements in $\mathbb{F}_{2^k}$ [Section \ref{sec:gao-precmp}]\\
			GM PCL2 & $3 \cdot 2^m - m - 3$ elements in $\mathbb{F}_{2^k}$ [Section \ref{sec:gao-precmp}]\\
			GM CO PCL2 & $2^m$ elements in $\mathbb{F}_{2^k}$ [Section \ref{sec:gao-precmp}]\\
			% Add more rows as needed
			\bottomrule
	\end{tabular}
%}
	\label{tab:precmp_comparison}
\end{table}


We implemented the all variants of the Cantor and Gao-Mateer algorithms in \CC except Gao-Mateer which has been implemented in the libiop library \cite{libiop}. We used the same library, libff \cite{libff}, for finite field operations as in libiop. We selected the field $\mathbb{F}_{2^{256}}$ for our experiments. Each field element is represented using four 64-bit limbs. Field multiplication is performed using 16 carry-less multiplication (CLMUL) instructions, commonly available on many x86 processors~\cite{gueron2010intel}, following a Karatsuba-like method for multiplying 4-limb values~\cite{libiop}.

The benchmarks were conducted on a system featuring an AMD Ryzen 9 9950x @ 5.7 CPU, equipped with 64 GB of DDR5 RAM. The CPU governor was set to \textit{Performance}, to ensure the CPU operates at its maximum clock speed throughout testing, thus minimizes fluctuations caused by power-saving mechanisms.
We executed each algorithm for 1000 different random polynomials \( f(x) \in \mathbb{F}_{2^{256}}[x] \) of degree less than \( n = 2^m \), where \( 4 \leq m \leq 28 \).

Figure~\ref{fig:benchmark} compares the runtime of the FFT algorithms. It shows that the time savings in Cantor PC are consistent across different input lengths. However, for Gao PCL1, Gao PCL2, and Gao CO PCL2, the savings become insignificant for larger $m$. The reason is that GM requires extensive memory access for Taylor expansion, the \textsf{Aggregate} module, and bit-reversal rearrangements, which suppresses the savings gained from the precomputations.  Figure~\ref{fig:sub_parts_normalized} depicts the fraction of each sub-algorithm in GM.

\begin{figure}
	\centering
	\resizebox{\textwidth}{!}{
		\input{Plots/plot_FFT}
	}
	\caption{Performance comparison of variances of Gao-Mateer and Cantor additive FFT algorithms of length $n=2^m$ for $4\leq m \leq 28$. The left plot is in a logarithmic scale. }
	\label{fig:benchmark}
\end{figure}

\begin{figure}[ht!]
	\centering
	\resizebox{.9\textwidth}{!}{
		\input{Plots/plot_GM_parts_normalized}
	}
	\caption{The fraction of execution time for each sub-algorithm in the Gao-Mateer FFT algorithm over $\mathbb{F}_{2^k}$ implemented in \cite{libiop}. The execution times of the Taylor expansion and bit-reversal sub-algorithms increase more rapidly with the input size.  \textit{Initialization} is a one-time computing for copying the input polynomial to a new vector. \textit{Span} and \textit{Merging} are the \textsf{Span} function and the nested for loop in \textsf{Aggregate} Module}
	\label{fig:sub_parts_normalized}
\end{figure}

\section{Summary} \label{Sec:Summary}
This work demonstrates the effectiveness of leveraging the Cantor special basis to enable the use of the Cantor additive FFT algorithm in post-quantum secure zkSNARKs, with a focus on Aurora~\cite{Aurora2019}. Our implementation shows that replacing the Gao-Mateer FFT with the Cantor FFT results in a significant reduction in computation time. Furthermore, we present a detailed theoretical analysis of the computational costs of the Cantor FFT, including exact counts of additions and multiplications, and an evaluation of FFT call complexity within the encoding of the R1CS in Aurora, based on the number of constraints, variables, and the target security parameter. Additionally, we introduce optimized building blocks for the Cantor FFT implementation and propose precomputation techniques that reduce overhead in both the Cantor and Gao-Mateer FFT algorithms when the basis of the affine subspaces is predetermined.

